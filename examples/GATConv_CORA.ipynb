{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqK8sAz1zqFm"
      },
      "source": [
        "# Install and import libraries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b4kGLvByrVs",
        "outputId": "632fa935-6205-4b3f-a252-53cf0ffa6572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting haiku-geometric==0.0.2\n",
            "  Downloading haiku_geometric-0.0.2-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax in /usr/local/lib/python3.8/dist-packages (from haiku-geometric==0.0.2) (0.3.25)\n",
            "Collecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.9-py3-none-any.whl (352 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from haiku-geometric==0.0.2) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from haiku-geometric==0.0.2) (57.4.0)\n",
            "Collecting jraph\n",
            "  Downloading jraph-0.0.6.dev0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax) (0.3.25+cuda11.cudnn805)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from optax) (4.4.0)\n",
            "Collecting chex>=0.1.5\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from optax) (1.21.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax->haiku-geometric==0.0.2) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax->haiku-geometric==0.0.2) (1.7.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from dm-haiku->haiku-geometric==0.0.2) (0.8.10)\n",
            "Collecting jmp>=0.0.2\n",
            "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: jmp, dm-haiku, jraph, chex, optax, haiku-geometric\n",
            "Successfully installed chex-0.1.5 dm-haiku-0.0.9 haiku-geometric-0.0.2 jmp-0.0.2 jraph-0.0.6.dev0 optax-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install haiku-geometric optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hcur1rPjzmr9"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import haiku as hk\n",
        "from haiku_geometric.nn import GCNConv, GATConv\n",
        "from haiku_geometric.datasets import Planetoid\n",
        "from haiku_geometric.transforms import normalize_features\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fsWkxRbB2Gqa"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import pickle\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from haiku_geometric.datasets.base import DataGraphTuple, GraphDataset\n",
        "from haiku_geometric.datasets.utils import download_url, extract_zip\n",
        "\n",
        "\n",
        "class Planetoid(GraphDataset):\n",
        "    r\"\"\"The Planetoid dataset from the `\"Revisiting Semi-Supervised Learning with Graph Embeddings\"\n",
        "    <https://arxiv.org/abs/1603.08861>`_ paper.\n",
        "\n",
        "    Parameters:\n",
        "        name (str): Name of the dataset. Can be one of ``'cora'``, ``'citeseer'`` or ``'pubmed'``.\n",
        "        root (str): Root directory where the dataset will be saved.\n",
        "        split (str): Which split to use. Can be one of ``'public'``, ``'full'`` or ``'random'``.\n",
        "        num_train_per_class (int): Number of training examples for the ``'random'`` split.\n",
        "        num_val (int): Number of validation examples. Only used for the ``'random'`` split.\n",
        "        num_test (int): Number of test examples. Only used for the ``'random'`` split.\n",
        "\n",
        "    **Attributes:**\n",
        "\n",
        "        - **data**: (List[DataGraphTuple]): List of graph tuples (in this case only one graph).\n",
        "        - **train_mask**: (List[bool]): Boolean mask for the training set.\n",
        "        - **val_mask**: (List[bool]): Boolean mask for the validation set.\n",
        "        - **test_mask**: (List[bool]): Boolean mask for the test set.\n",
        "        - **num_classes**: (int): Number of classes.\n",
        "\n",
        "    Stats:\n",
        "        .. list-table::\n",
        "            :widths: 10 10 10 10 10\n",
        "            :header-rows: 1\n",
        "\n",
        "            * - Name\n",
        "              - #nodes\n",
        "              - #edges\n",
        "              - #node features\n",
        "              - #classes\n",
        "            * - Cora\n",
        "              - 2,708\n",
        "              - 10,858\n",
        "              - 1,433\n",
        "              - 7\n",
        "            * - CiteSeer\n",
        "              - 3,312\n",
        "              - 9,464\n",
        "              - 3,703\n",
        "              - 6\n",
        "            * - PubMed\n",
        "              - 19,717\n",
        "              - 88,676\n",
        "              - 500\n",
        "              - 3\n",
        "    \n",
        "    \"\"\"\n",
        "    def _download_planetoid(self, dataset, folder):\n",
        "        URL = \"https://github.com/kimiyoung/planetoid/raw/master/data/\"\n",
        "\n",
        "        NAMES = ['x', 'y', 'tx', 'ty', 'graph', 'allx', 'ally', 'test.index']\n",
        "        OBJECTS = []\n",
        "        for i in range(len(NAMES)):\n",
        "            download_url(f\"{URL}ind.{dataset}.{NAMES[i]}\", folder=folder, filename=None)\n",
        "            if NAMES[i] == 'test.index':\n",
        "                fb = open(folder + \"ind.{}.{}\".format(dataset, NAMES[i]), 'r')\n",
        "                OBJECTS.append([int(x) for x in fb.readlines()])\n",
        "            else:\n",
        "                fb = open(folder + \"ind.{}.{}\".format(dataset, NAMES[i]), 'rb')\n",
        "                OBJECTS.append(pickle.load(fb, encoding='latin1'))\n",
        "        return tuple(OBJECTS)\n",
        "\n",
        "\n",
        "    def _senders_receivers_from_dict(self, graph_dict):\n",
        "        row, col = [], []\n",
        "        for key, value in graph_dict.items():\n",
        "            row += [key] * len(value)\n",
        "            col += value\n",
        "        #: TODO: remove self edges?\n",
        "        return jnp.asarray(row), jnp.asarray(col)\n",
        "\n",
        "\n",
        "    def _process_planetoid_data(self, x, y, tx, ty, graph, allx, ally, test_index):\n",
        "        train_index = jnp.arange(y.shape[0], dtype=jnp.int32)\n",
        "        val_index = jnp.arange(y.shape[0], y.shape[0] + 500, dtype=jnp.int32)\n",
        "        test_index = jnp.array(test_index)\n",
        "        sorted_test_index = jnp.sort(test_index)\n",
        "\n",
        "        x = jnp.array(x.toarray())\n",
        "        tx = jnp.array(tx.toarray())\n",
        "        allx = jnp.array(allx.toarray())\n",
        "\n",
        "        nx = jnp.concatenate([allx, tx], axis=0)\n",
        "        ny = jnp.concatenate([ally, ty], axis=0).argmax(axis=1)\n",
        "\n",
        "        nx = nx.at[test_index].set(nx[sorted_test_index])\n",
        "        ny = ny.at[test_index].set(ny[sorted_test_index])\n",
        "\n",
        "        def sample_mask(index, num_nodes):\n",
        "            mask = jnp.zeros((num_nodes, ), dtype=jnp.uint8)\n",
        "            mask = mask.at[index].set(1)\n",
        "            mask = mask.astype(jnp.bool_)\n",
        "            return mask\n",
        "\n",
        "        train_mask = sample_mask(train_index, num_nodes=ny.shape[0])\n",
        "        val_mask = sample_mask(val_index, num_nodes=ny.shape[0])\n",
        "        test_mask = sample_mask(test_index, num_nodes=ny.shape[0])\n",
        "\n",
        "        senders, receivers = self._senders_receivers_from_dict(graph)\n",
        "\n",
        "        graph = DataGraphTuple(\n",
        "            nodes=nx,\n",
        "            senders=senders,\n",
        "            receivers=receivers,\n",
        "            edges=None,\n",
        "            n_node=ny.shape[0],\n",
        "            n_edge=senders.shape[0],\n",
        "            globals=None,\n",
        "            y=ny,\n",
        "            train_mask=None,\n",
        "            position=None\n",
        "        )\n",
        "\n",
        "        train_mask = train_mask\n",
        "        val_mask = val_mask\n",
        "        test_mask = test_mask\n",
        "        num_classes = ally.shape[1]\n",
        "        return graph, train_mask, val_mask, test_mask, num_classes\n",
        "    \n",
        "    def __init__(self, name: str, root: str, split: str = \"public\",\n",
        "                 num_train_per_class: int = 20, num_val: int = 500,\n",
        "                 num_test: int = 1000):\n",
        "        x, y, tx, ty, graph, allx, ally, test_index = self._download_planetoid(name, root)\n",
        "        graph, train_mask, val_mask, test_mask, num_classes \\\n",
        "                = self._process_planetoid_data(x, y, tx, ty, graph, allx, ally, test_index)\n",
        "        super().__init__([graph])\n",
        "        self.train_mask = train_mask\n",
        "        self.val_mask = val_mask\n",
        "        self.test_mask = test_mask\n",
        "        self.num_val = num_val\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        split = split.lower()\n",
        "        assert split in ['public', 'full', 'random']\n",
        "\n",
        "        if split == 'full':\n",
        "            self.train_mask = jnp.full(self.train_mask.shape, 1)\n",
        "            self.train_mask.at[self.val_mask | self.test_mask].set(0)\n",
        "\n",
        "        elif split == 'random':\n",
        "            self.train_mask = jnp.full(self.train_mask.shape, 0)\n",
        "            for c in range(self.num_classes):\n",
        "                idx = jnp.nonzero(self.y == c)[0]\n",
        "                idx = idx[\n",
        "                    jax.random.permutation(\n",
        "                        jax.random.PRNGKey(42), idx.shape[0])[:num_train_per_class]]\n",
        "                self.train_mask.at[idx].set(1)\n",
        "\n",
        "            remaining = jnp.nonzero(~self.train_mask)[0]\n",
        "            remaining = remaining[jax.random.permutation(\n",
        "                        jax.random.PRNGKey(42), remaining.shape[0])]\n",
        "\n",
        "            self.val_mask = jnp.full(self.val_mask.shape, 0)\n",
        "            self.val_mask.at[remaining[:num_val]].set(1)\n",
        "\n",
        "            self.test_mask = jnp.full(self.test_mask.shape, 0)\n",
        "            self.test_mask.at[remaining[:num_val]].set(1)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def dropout(key, rate, x):\n",
        "    keep = 1.0 - rate \n",
        "    binary_value = jax.random.uniform(key, x.shape) < keep\n",
        "    res = jnp.multiply(x, binary_value)\n",
        "    res /= keep\n",
        "    return res\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "d0blRtYkOK-W",
        "outputId": "ce2adbdc-ff70-4378-97e4-9e054ff07b30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef dropout(key, rate, x):\\n    keep = 1.0 - rate \\n    binary_value = jax.random.uniform(key, x.shape) < keep\\n    res = jnp.multiply(x, binary_value)\\n    res /= keep\\n    return res\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sgKAA1a_89kr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.tree_util as tree\n",
        "import jraph\n",
        "\n",
        "from typing import Optional, Union\n",
        "from haiku_geometric.nn.aggr.utils import aggregation\n",
        "from haiku_geometric.nn.conv.utils import validate_input\n",
        "from haiku_geometric.transforms import add_self_loops\n",
        "\n",
        "\n",
        "class GATConv(hk.Module):\n",
        "    r\"\"\"Graph attention layer from `\"Graph Attention Networks\"\n",
        "    <https://arxiv.org/abs/1710.10903>`_ paper\n",
        "\n",
        "    where each node's output feature is computed as follows:\n",
        "    \n",
        "    .. math::\n",
        "        \\vec{h}_{i}^{\\prime}=\\sigma\\left(\\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j} \\mathbf{W} \\vec{h}_{j}\\right)\n",
        "\n",
        "    where the attention coefficients are computed as:\n",
        "    \n",
        "    .. math::\n",
        "        \\alpha_{i j}=\\frac{\\exp \\left(\\operatorname{LeakyReLU}\\left(\\overrightarrow{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}_{i} \\| \\mathbf{W} \\vec{h}_{j}\\right]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(\\operatorname{LeakyReLU}\\left(\\overrightarrow{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}_{i} \\| \\mathbf{W} \\vec{h}_{k}\\right]\\right)\\right)}\n",
        "\n",
        "    When multiple attention heads are used, the output nodes features are averaged:\n",
        "    \n",
        "    .. math::\n",
        "        \\vec{h}_{i}^{\\prime}=\\sigma\\left(\\frac{1}{K} \\sum_{k=1}^{K} \\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j}^{k} \\mathbf{W}^{k} \\vec{h}_{j}\\right)\n",
        "\n",
        "    If `concat=True` the output feature is the concatenation of the :math:`K` heads features:\n",
        "    \n",
        "    .. math::\n",
        "        \\vec{h}_{i}^{\\prime}=\\|_{k=1}^{K} \\sigma\\left(\\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j}^{k} \\mathbf{W}^{k} \\vec{h}_{j}\\right)\n",
        "\n",
        "    Args:\n",
        "        out_channels (int): Size of the output features produced by the layer for each node.\n",
        "        heads (int, optional): Number of head attentions.\n",
        "            (default: :obj:`1`)\n",
        "        concat (bool, optional): If :obj:`False`, the multi-head features are averaged\n",
        "            else concatenated.\n",
        "            (default: :obj:`True`)\n",
        "        negative_slope (float, optional): scalar specifying the negative slope of the LeakyReLU.\n",
        "            (default: :obj:`0.2`)\n",
        "        add_self_loops (bool, optional): If :obj:`True`, will add\n",
        "            a self-loop for each node of the graph. (default: :obj:`True`)\n",
        "        dropout (float, optional): Dropout applied to attention weights.\n",
        "            This dropout simulates random sampling of the neigbours.\n",
        "            (default: :obj:`0.0`)\n",
        "        dropout_nodes (float, optional): Dropout applied initially to the input features.\n",
        "            (default: :obj:`0.0`)\n",
        "        bias (bool, optional): If :obj:`True`, the layer will add\n",
        "            an additive bias to the output. (default: :obj:`True`)\n",
        "        init (hk.initializers.Initializer): Weights initializer\n",
        "            (default: :obj:`hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"truncated_normal\")`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            out_channels: int,\n",
        "            heads: int = 1,\n",
        "            concat: bool = True,\n",
        "            negative_slope: float = 0.2,\n",
        "            dropout: float = 0.0,\n",
        "            dropout_nodes: float = 0.0,\n",
        "            add_self_loops: bool = True,\n",
        "            # edge_dim: Optional[int] = None, # TODO: include edges in GATConv\n",
        "            # fill_value: Union[float, Tensor, str] = 'mean',\n",
        "            bias: bool = True,\n",
        "            init: hk.initializers.Initializer = None\n",
        "    ):\n",
        "        \"\"\"\"\"\"\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.concat = concat\n",
        "        self.dropout_attention = dropout\n",
        "        self.dropout_nodes = dropout_nodes\n",
        "        self.negative_slope = negative_slope\n",
        "        self.add_self_loops = add_self_loops\n",
        "\n",
        "        # Initialize parameters\n",
        "        C = self.out_channels\n",
        "        H = self.heads\n",
        "\n",
        "        if init is None:\n",
        "          init = hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"truncated_normal\")\n",
        "\n",
        "        self.linear_proj = hk.Linear(C * H, with_bias=False, \n",
        "                                     w_init=init)\n",
        "\n",
        "        self.scoring_fn_target = hk.get_parameter(\n",
        "            \"scoring_fn_target\", \n",
        "            shape=[1, H, C], \n",
        "            init=init)\n",
        "        self.scoring_fn_source = hk.get_parameter(\n",
        "            \"scoring_fn_source\", \n",
        "            shape=[1, H, C], \n",
        "            init=init)\n",
        "\n",
        "    def __call__(self,\n",
        "                 nodes: jnp.ndarray = None,\n",
        "                 senders: jnp.ndarray = None,\n",
        "                 receivers: jnp.ndarray = None,\n",
        "                 edges: Optional[jnp.ndarray] = None,\n",
        "                 graph: Optional[jraph.GraphsTuple] = None,\n",
        "                 training: bool = False\n",
        "                 ) -> Union[jnp.ndarray, jraph.GraphsTuple]:\n",
        "        \"\"\"\"\"\"\n",
        "        in_nodes_features, edges, receivers, senders = \\\n",
        "            validate_input(nodes, senders, receivers, edges, graph)\n",
        "\n",
        "        C = self.out_channels\n",
        "        H = self.heads\n",
        "\n",
        "        try:\n",
        "            sum_n_node = in_nodes_features.shape[0]\n",
        "        except IndexError:\n",
        "            raise IndexError('GATConv requires node features')\n",
        "\n",
        "        # reshape to : (N, H, C)\n",
        "        nodes_features_proj = self.linear_proj(in_nodes_features).reshape(-1, H, C)\n",
        "\n",
        "        if training:\n",
        "            nodes_features_proj = hk.dropout(\n",
        "                jax.random.PRNGKey(42), self.dropout_nodes, nodes_features_proj)\n",
        "\n",
        "        total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
        "        if self.add_self_loops:\n",
        "            # We add self edges to the senders and receivers so that each node\n",
        "            # includes itself in aggregation.\n",
        "            #   receivers (or senders) shape = (|edges|, 1)\n",
        "            receivers, senders = add_self_loops(receivers, senders,\n",
        "                                                   total_num_nodes)\n",
        "\n",
        "        # shape: (N, H)\n",
        "        scores_source = jnp.sum(nodes_features_proj * self.scoring_fn_source, axis=-1)\n",
        "        scores_target = jnp.sum(nodes_features_proj * self.scoring_fn_target, axis=-1)\n",
        "\n",
        "        # scores_source_lifted shape: (|edges|, H)\n",
        "        # nodes_features_proj shape: (|edges|, H, C)\n",
        "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted \\\n",
        "            = self.lift(scores_source, scores_target, nodes_features_proj, senders, receivers)\n",
        "\n",
        "        # shape: (|edges|, 1)\n",
        "        scores_per_edge = jax.nn.leaky_relu(\n",
        "            (scores_source_lifted + scores_target_lifted), \n",
        "            negative_slope=self.negative_slope)\n",
        "\n",
        "        # shape: (|edges|, 1)\n",
        "        attentions_per_edge = jraph.segment_softmax(scores_per_edge, receivers, num_segments=sum_n_node)\n",
        "\n",
        "        if training:\n",
        "            attentions_per_edge = hk.dropout(\n",
        "                jax.random.PRNGKey(42), self.dropout_attention, attentions_per_edge)\n",
        "\n",
        "        # shape: (|edges|, H, C)\n",
        "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * jnp.expand_dims(attentions_per_edge, axis=-1)\n",
        "\n",
        "        # shape: (N, H, C)\n",
        "        out_nodes_features = jax.ops.segment_sum(nodes_features_proj_lifted_weighted, receivers, num_segments=sum_n_node)\n",
        "\n",
        "        if self.concat:\n",
        "            out_nodes_features = jnp.reshape(out_nodes_features, (-1, H * C))\n",
        "        else:\n",
        "            out_nodes_features = jnp.mean(out_nodes_features, axis=1)\n",
        "\n",
        "        if graph is not None:\n",
        "            graph = graph._replace(nodes=out_nodes_features)\n",
        "            return graph\n",
        "        else:\n",
        "            return out_nodes_features\n",
        "\n",
        "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, senders, receivers):\n",
        "        src_nodes_index = senders\n",
        "        trg_nodes_index = receivers\n",
        "\n",
        "        scores_source = scores_source[src_nodes_index]\n",
        "        scores_target = scores_target[trg_nodes_index]\n",
        "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj[src_nodes_index]\n",
        "\n",
        "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn-0V7uzzvfF"
      },
      "source": [
        "# Inspecting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBmAmUEBzwMl",
        "outputId": "1a4d3695-5c50-474e-c0e7-7ffb3a6a5fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ],
      "source": [
        "NAME = 'cora'\n",
        "FOLDER = '/tmp/cora/'\n",
        "dataset = Planetoid(NAME, FOLDER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpmNbY0o1KfV",
        "outputId": "155196ad-cbd9-4ed4-ef96-02e770d4b12e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of graphs : 1\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of graphs :\", len(dataset.data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI1qS2Un1QdR",
        "outputId": "7e729e91-a435-4ba9-efb5-1aa1dd6d74b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes : 2708\n",
            "Number of edges : 10858\n",
            "Nodes features size : 1433\n"
          ]
        }
      ],
      "source": [
        "graph = dataset.data[0]\n",
        "print(\"Number of nodes :\", graph.n_node)\n",
        "print(\"Number of edges :\", graph.n_edge)\n",
        "print(\"Nodes features size :\", graph.nodes.shape[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8VTBH4M2LiR",
        "outputId": "ab612c37-e59e-4825-8065-ee9afefd1215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples:  140\n",
            "Validation samples:  500\n",
            "Test samples:  1000\n"
          ]
        }
      ],
      "source": [
        "train_mask = dataset.train_mask\n",
        "val_mask = dataset.val_mask\n",
        "test_mask = dataset.test_mask\n",
        "\n",
        "print(\"Train samples: \", jnp.count_nonzero(train_mask))\n",
        "print(\"Validation samples: \", jnp.count_nonzero(val_mask))\n",
        "print(\"Test samples: \", jnp.count_nonzero(test_mask))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will need these later during training\n",
        "train_labels = graph.y[train_mask]\n",
        "val_labels = graph.y[val_mask]\n",
        "test_labels = graph.y[test_mask]"
      ],
      "metadata": {
        "id": "iMD3HRfqJLf5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acq4QJOh3Cd3",
        "outputId": "ec908fbf-df05-4feb-eb6c-a268d454e137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes:  7\n"
          ]
        }
      ],
      "source": [
        "NUM_CLASSES = len(jnp.unique(graph.y))\n",
        "print(\"Number of classes: \", NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ruJ_vdwvyMvy"
      },
      "outputs": [],
      "source": [
        "# Features are normalized\n",
        "graph = graph._replace(nodes = normalize_features(graph.nodes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEd7ict92sNE"
      },
      "source": [
        "# Define GAT model\n",
        "We create here a model with 2 layers of [GATConv](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.conv.GATConv) from the [\"Graph Attention Networks\"](https://arxiv.org/abs/1710.10903) paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7sdva_HU2uFI"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = len(jnp.unique(graph.y))\n",
        "\n",
        "# Hyperparameters\n",
        "args = {\n",
        "    'hidden_dim': 8,\n",
        "    'output_dim': NUM_CLASSES,\n",
        "    'heads': 8,\n",
        "    'dropout_attention': 0.15,\n",
        "    'dropout_nodes': 0.00,\n",
        "    'num_steps': 500,\n",
        "    'learning_rate': 1e-3,\n",
        "    'weight_decay': 0.1,\n",
        "    'initializer': hk.initializers.VarianceScaling(1.0, \"fan_avg\", \"truncated_normal\") # glorot (truncated)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iZlvjsto3f5n"
      },
      "outputs": [],
      "source": [
        "class MyNet(hk.Module):\n",
        "  def __init__(self, hidden_dim, output_dim, heads, dropout_attention, dropout_nodes, init):\n",
        "    super().__init__()\n",
        "    self.dropout_attention = dropout_attention\n",
        "    self.dropout_nodes = dropout_nodes\n",
        "    self.conv1 = GATConv(hidden_dim, heads=heads, \n",
        "                         dropout=dropout_attention, \n",
        "                         dropout_nodes=dropout_nodes,\n",
        "                         init=init)\n",
        "    self.conv2 = GATConv(output_dim, heads=1, concat=False,\n",
        "                         dropout=dropout_attention, \n",
        "                         dropout_nodes=dropout_nodes, \n",
        "                         init=init)\n",
        "\n",
        "  def __call__(self, graph, training):\n",
        "    nodes, senders, receivers = graph.nodes, graph.senders, graph.receivers\n",
        "    x = nodes\n",
        "\n",
        "    if training:\n",
        "      x = hk.dropout(jax.random.PRNGKey(42), self.dropout_nodes, x)  \n",
        "    x = self.conv1(x, senders, receivers, training=training)\n",
        "    x = jax.nn.elu(x) \n",
        "    \n",
        "    if training:\n",
        "      x = hk.dropout(jax.random.PRNGKey(42), self.dropout_nodes, x)  \n",
        "    x = self.conv2(x, senders, receivers, training=training)\n",
        "    x = jax.nn.softmax(x) # as in the original implementation\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def forward(graph, training, args):\n",
        "  module = MyNet(\n",
        "      args['hidden_dim'], \n",
        "      args['output_dim'],\n",
        "      args['heads'],\n",
        "      args['dropout_attention'],\n",
        "      args['dropout_nodes'],\n",
        "      args['initializer'],\n",
        "  )\n",
        "  return module(graph, training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvemQdQf9RPt"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EHOv2ts9PwF"
      },
      "source": [
        "Transform Haiku module"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rng_key = jax.random.PRNGKey(42)\n",
        "model = hk.without_apply_rng(hk.transform(forward))\n",
        "params = model.init(graph=graph, training=True, args=args, rng=rng_key)\n",
        "output = model.apply(graph=graph, training=True, args=args, params=params)"
      ],
      "metadata": {
        "id": "zd_a0iJHJvCP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iBDgioMPtVB"
      },
      "source": [
        "Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSX2VEyx3Hm3",
        "outputId": "09109b98-a2f8-493e-d3af-b50221503cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train accuracy: 0.42  Val accuracy 0.31\n",
            "Epoch 10 Train accuracy: 0.96  Val accuracy 0.69\n",
            "Epoch 20 Train accuracy: 0.97  Val accuracy 0.72\n",
            "Epoch 30 Train accuracy: 0.97  Val accuracy 0.72\n",
            "Epoch 40 Train accuracy: 0.97  Val accuracy 0.72\n",
            "Epoch 50 Train accuracy: 0.96  Val accuracy 0.72\n",
            "Epoch 60 Train accuracy: 0.96  Val accuracy 0.72\n",
            "Epoch 70 Train accuracy: 0.96  Val accuracy 0.72\n",
            "Epoch 80 Train accuracy: 0.96  Val accuracy 0.72\n",
            "Epoch 90 Train accuracy: 0.96  Val accuracy 0.72\n",
            "Epoch 100 Train accuracy: 0.96  Val accuracy 0.72\n",
            "Epoch 110 Train accuracy: 0.98  Val accuracy 0.73\n",
            "Epoch 120 Train accuracy: 0.98  Val accuracy 0.73\n",
            "Epoch 130 Train accuracy: 0.98  Val accuracy 0.73\n",
            "Epoch 140 Train accuracy: 0.98  Val accuracy 0.74\n",
            "Epoch 150 Train accuracy: 0.97  Val accuracy 0.75\n",
            "Epoch 160 Train accuracy: 0.97  Val accuracy 0.76\n",
            "Epoch 170 Train accuracy: 0.97  Val accuracy 0.77\n",
            "Epoch 180 Train accuracy: 0.97  Val accuracy 0.77\n",
            "Epoch 190 Train accuracy: 0.97  Val accuracy 0.78\n",
            "Epoch 200 Train accuracy: 0.97  Val accuracy 0.79\n",
            "Epoch 210 Train accuracy: 0.97  Val accuracy 0.79\n",
            "Epoch 220 Train accuracy: 0.97  Val accuracy 0.79\n",
            "Epoch 230 Train accuracy: 0.97  Val accuracy 0.79\n",
            "Epoch 240 Train accuracy: 0.97  Val accuracy 0.79\n",
            "Epoch 250 Train accuracy: 0.97  Val accuracy 0.79\n",
            "Epoch 260 Train accuracy: 0.98  Val accuracy 0.78\n",
            "Epoch 270 Train accuracy: 0.98  Val accuracy 0.78\n",
            "Epoch 280 Train accuracy: 0.99  Val accuracy 0.78\n",
            "Epoch 290 Train accuracy: 0.99  Val accuracy 0.78\n",
            "Epoch 300 Train accuracy: 0.99  Val accuracy 0.78\n",
            "Epoch 310 Train accuracy: 0.99  Val accuracy 0.78\n",
            "Epoch 320 Train accuracy: 0.99  Val accuracy 0.78\n",
            "Epoch 330 Train accuracy: 1.00  Val accuracy 0.77\n",
            "Epoch 340 Train accuracy: 1.00  Val accuracy 0.77\n",
            "Epoch 350 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 360 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 370 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 380 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 390 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 400 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 410 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 420 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 430 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 440 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 450 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 460 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 470 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 480 Train accuracy: 1.00  Val accuracy 0.76\n",
            "Epoch 490 Train accuracy: 1.00  Val accuracy 0.75\n"
          ]
        }
      ],
      "source": [
        "@jax.jit\n",
        "def prediction_loss(params):\n",
        "    logits = model.apply(params=params, graph=graph, training=True, args=args)\n",
        "    logits = logits[train_mask]\n",
        "    one_hot_labels = jax.nn.one_hot(train_labels, NUM_CLASSES)\n",
        "    loss = jnp.sum(optax.softmax_cross_entropy(logits, one_hot_labels))\n",
        "    #jax.debug.print(\"loss {loss}\", loss=loss)\n",
        "    return loss\n",
        "\n",
        "opt_init, opt_update = optax.adamw(args[\"learning_rate\"], weight_decay=args[\"weight_decay\"])\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "@jax.jit\n",
        "def update(params, opt_state):\n",
        "    g = jax.grad(prediction_loss)(params)\n",
        "    updates, opt_state = opt_update(g, opt_state, params=params)\n",
        "    return optax.apply_updates(params, updates), opt_state\n",
        "\n",
        "@jax.jit\n",
        "def accuracy_train(params):\n",
        "    decoded_nodes = model.apply(params=params,  graph=graph, training=False, args=args)\n",
        "    decoded_nodes = decoded_nodes[train_mask]\n",
        "    return jnp.mean(jnp.argmax(decoded_nodes, axis=1) == train_labels)\n",
        "\n",
        "@jax.jit\n",
        "def accuracy_val(params):\n",
        "    decoded_nodes = model.apply(params=params, graph=graph, training=False, args=args)\n",
        "    decoded_nodes = decoded_nodes[val_mask]\n",
        "    return jnp.mean(jnp.argmax(decoded_nodes, axis=1) == val_labels)\n",
        "\n",
        "best_acc = 0.0\n",
        "best_model_params = None\n",
        "for step in range(args['num_steps']):\n",
        "    params, opt_state = update(params, opt_state)\n",
        "    val_acc = accuracy_val(params).item()\n",
        "    if val_acc > best_acc:\n",
        "      best_acc = val_acc\n",
        "      best_model_params = copy.copy(params)\n",
        "    if step % 10 == 0:\n",
        "      print(f\"Epoch {step} Train accuracy: {accuracy_train(params).item():.2f} \"\n",
        "          f\" Val accuracy {val_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "abgyk8qkxTin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58692705-2025-4ab5-de8e-9363703bb120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy 0.79\n"
          ]
        }
      ],
      "source": [
        "@jax.jit\n",
        "def test_f(params):\n",
        "    decoded_nodes = model.apply(params=params, graph=graph, training=False, args=args)\n",
        "    decoded_nodes = decoded_nodes[test_mask]\n",
        "    return jnp.mean(jnp.argmax(decoded_nodes, axis=1) == test_labels)\n",
        "\n",
        "print(f\"Test accuracy {test_f(params).item():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad but needs more regularization / fine tuning!"
      ],
      "metadata": {
        "id": "lNGveYI-hXmz"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}