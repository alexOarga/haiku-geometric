{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lla34xIY64Hh"
      },
      "source": [
        "# Top-k pooling and graph convolution trained on PROTEINS dataset with Haiku Geometric\n",
        "\n",
        "This notebook contains an example on how to use [Haiku Geometric](https://github.com/alexOarga/haiku-geometric) to create graph convolutional networks, graph pooling layers and train them on the PROTEINS dataset.\n",
        "\n",
        "[Haiku Geometric](https://github.com/alexOarga/haiku-geometric) is a graph neural network library built for [JAX](https://github.com/google/jax) + [Haiku](https://github.com/deepmind/dm-haiku).\n",
        "\n",
        "If wou want to know more about Haiku Geometric, please visit the [documentation](https://haiku-geometric.readthedocs.io/en/latest/).\n",
        "You can find there a more detailed explanation of the library and how to use it as well as the API reference.\n",
        "\n",
        "If you want to see other examples on how to use Haiku Geometric to build other\n",
        "graph neural networks, check out the [examples](https://haiku-geometric.readthedocs.io/en/latest/examples.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XxejpLi73VF"
      },
      "source": [
        "# 1. Install and import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n8AqiU0piCo",
        "outputId": "035124a8-bc4d-485b-e6f0-461d6c79b060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax) (0.1.7)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.14)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.14+cuda11.cudnn86)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from optax) (1.23.5)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (4.5.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (1.11.2)\n",
            "Collecting git+https://github.com/alexOarga/haiku-geometric.git\n",
            "  Cloning https://github.com/alexOarga/haiku-geometric.git to /tmp/pip-req-build-3di1ncv3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/alexOarga/haiku-geometric.git /tmp/pip-req-build-3di1ncv3\n",
            "  Resolved https://github.com/alexOarga/haiku-geometric.git to commit 5e1dd4ffc6de467a9f66b2e9485b7b64863b2409\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from haiku-geometric==0.0.4) (0.4.14)\n",
            "Collecting dm-haiku (from haiku-geometric==0.0.4)\n",
            "  Downloading dm_haiku-0.0.10-py3-none-any.whl (360 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.3/360.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jraph (from haiku-geometric==0.0.4)\n",
            "  Downloading jraph-0.0.6.dev0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from haiku-geometric==0.0.4) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haiku-geometric==0.0.4) (4.66.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from haiku-geometric==0.0.4) (0.56.4)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->haiku-geometric==0.0.4) (1.4.0)\n",
            "Collecting jmp>=0.0.2 (from dm-haiku->haiku-geometric==0.0.4)\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->haiku-geometric==0.0.4) (1.23.5)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->haiku-geometric==0.0.4) (0.9.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->haiku-geometric==0.0.4) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->haiku-geometric==0.0.4) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax->haiku-geometric==0.0.4) (1.11.2)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from jraph->haiku-geometric==0.0.4) (0.4.14+cuda11.cudnn86)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->haiku-geometric==0.0.4) (0.39.1)\n",
            "Building wheels for collected packages: haiku-geometric\n",
            "  Building wheel for haiku-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for haiku-geometric: filename=haiku_geometric-0.0.4-py3-none-any.whl size=75377 sha256=40d1cefbc0c0445573efc0746354256fa6d7a38dfc86e39c6d8d5a5501891504\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vkm21hum/wheels/6d/c8/78/14a6c0c286c4741751a879af965959215998cf61393802087a\n",
            "Successfully built haiku-geometric\n",
            "Installing collected packages: jmp, dm-haiku, jraph, haiku-geometric\n",
            "Successfully installed dm-haiku-0.0.10 haiku-geometric-0.0.4 jmp-0.0.4 jraph-0.0.6.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install optax\n",
        "!pip install git+https://github.com/alexOarga/haiku-geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "At7GEmL37y0v"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import haiku as hk\n",
        "import jax\n",
        "import time\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "from functools import partial\n",
        "from random import shuffle\n",
        "\n",
        "from haiku_geometric.utils import batch, pad_graph\n",
        "from haiku_geometric.nn import GraphConv, TopKPooling\n",
        "from haiku_geometric.nn.pool import global_mean_pool, global_max_pool\n",
        "from haiku_geometric.datasets import TUDataset\n",
        "\n",
        "gap = global_mean_pool\n",
        "gmp = global_max_pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdTvvCpV8NjW"
      },
      "source": [
        "# 2. Load and prepare dataset\n",
        "\n",
        "\n",
        "Here we will load the PROTEINS dataset from the [TU Dortmund](https://chrsmrrs.github.io/datasets/docs/datasets/). In this notebook we will simply use lists as in-memory datasets, but I encourage you to create your own custom data loader.\n",
        "\n",
        "First, we load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MxM2eQqe8QWU"
      },
      "outputs": [],
      "source": [
        "dataset = 'PROTEINS'\n",
        "folder = '.'\n",
        "\n",
        "tu_dataset = TUDataset(dataset, folder, use_node_attr=True)\n",
        "tu_dataset_len = int(len(tu_dataset.data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tsisv4SG_uew",
        "outputId": "6ecf496f-b43c-4913-9715-ce11946f9716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of graphs : 1113\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of graphs :\", tu_dataset_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTKkMvlR_aIF"
      },
      "source": [
        "We now shuffle the dataset and split the dataset into train, validation and test using 80%, 10% and 10% respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0qtCWJM98uB-"
      },
      "outputs": [],
      "source": [
        "for i in range(tu_dataset_len):\n",
        "    # ⚠️ Notice: We store each graph label in the 'globals' attribute\n",
        "    tu_dataset.data[i] = tu_dataset.data[i]._replace(globals=tu_dataset.y[i].reshape(1, ))\n",
        "\n",
        "# We shuffle the dataset randomly\n",
        "shuffle(tu_dataset.data)\n",
        "\n",
        "# Prepare the train-val-test splits\n",
        "train_size = int(tu_dataset_len * 0.8)\n",
        "val_size = int(tu_dataset_len * 0.1)\n",
        "test_size = int(tu_dataset_len - train_size - val_size)\n",
        "splits_range = {\n",
        "    'train': (0, train_size),\n",
        "    'val': (train_size, train_size + val_size),\n",
        "    'test': (train_size + val_size, tu_dataset_len)\n",
        "}\n",
        "\n",
        "train_dataset = tu_dataset.data[slice(*splits_range['train'])]\n",
        "val_dataset = tu_dataset.data[slice(*splits_range['val'])]\n",
        "test_dataset = tu_dataset.data[slice(*splits_range['test'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BykR2pvMAqdr"
      },
      "source": [
        "Let us inspect a couple graphs from the train split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d0yM9WuAu2N",
        "outputId": "33fd9ba1-8df0-468e-82b9-b5764b5a757f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph 0: Number of nodes: [26]\n",
            "Graph 0: Number of edges: [102]\n",
            "Graph 0: Nodes features size: 4\n",
            "Graph 0: Label: [1]\n",
            "    \n",
            "Graph 1: Number of nodes: [52]\n",
            "Graph 1: Number of edges: [206]\n",
            "Graph 1: Nodes features size: 4\n",
            "Graph 1: Label: [1]\n",
            "    \n",
            "Graph 2: Number of nodes: [20]\n",
            "Graph 2: Number of edges: [82]\n",
            "Graph 2: Nodes features size: 4\n",
            "Graph 2: Label: [1]\n",
            "    \n",
            "    ...\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "  graph = train_dataset[i]\n",
        "  print(f\"Graph {i}: Number of nodes:\", graph.n_node)\n",
        "  print(f\"Graph {i}: Number of edges:\", graph.n_edge)\n",
        "  print(f\"Graph {i}: Nodes features size:\", graph.nodes.shape[-1])\n",
        "  print(f\"Graph {i}: Label:\", graph.globals)\n",
        "  print(\"    \")\n",
        "print(\"    ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdrFXTzEBpuY"
      },
      "source": [
        "# 3. Define the model\n",
        "\n",
        "We are ready to define our model. First, we will apply a [GraphConv](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.conv.GraphConv) layer and then a [TopKPooling](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.pool.TopKPooling) layer. The top-k pooling layer will reduce the number of nodes in the graph with a reduction factor of 0.8. Finally, we apply both a [global_add_pool](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.pool.global_add_pool) and a [global_mean_pool](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.pool.global_mean_pool) layer and concatenate the output. We repeat this procedure 3 times.\n",
        "\n",
        "## 3.1 JIT-able TopKPooling\n",
        "\n",
        "The [TopKPooling](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.pool.TopKPooling) layer is not JIT-able by default, because it naturally works on arrays shaped dynamically. However, in order to fully benefit from the speed of JAX, we need to JIT the model, this is, we need to make the layer JIT-able. This can be done by using the `create_new_batch` parameter of the layer during the forward call. If this parameter is set to `True`, the layer does not remove nodes nor edges from the graph, but instead creates a new batch with the nodes that were removed (see image below). Additionally, the edges from the removed edges become self-loops (see image below).\n",
        "\n",
        "![topk-example-image](https://raw.githubusercontent.com/alexOarga/haiku-geometric/331953f617d55126fc57400b53a0af36199a6a3c/docs/source/_static/topk.png)\n",
        "\n",
        "In addition to the `create_new_batch` parameter, to make the layer fully jit-able\n",
        "we will need to provide as static parameter `batch_size`, this is, the numbers of graphs in a batch. This parameter is needed to create the new batch.\n",
        "\n",
        "## 3.2 Model\n",
        "\n",
        "With the above in mind, we are ready to create our model. Notice that we provide the parameters `create_new_batch` and `batch_size` to the TopKPooling layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UvH2cC4CTXdU"
      },
      "outputs": [],
      "source": [
        "class TopkNetwork(hk.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GraphConv(128)\n",
        "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
        "        self.conv2 = GraphConv(128)\n",
        "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
        "        self.conv3 = GraphConv(128)\n",
        "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
        "\n",
        "        self.lin1 = hk.Linear(128)\n",
        "        self.lin2 = hk.Linear(64)\n",
        "        self.lin3 = hk.Linear(2)\n",
        "\n",
        "    def __call__(self,\n",
        "                 nodes: jnp.ndarray = None,\n",
        "                 senders: jnp.ndarray = None,\n",
        "                 receivers: jnp.ndarray = None,\n",
        "                 edges: jnp.ndarray = None,\n",
        "                 batch: jnp.ndarray = None,\n",
        "                 create_new_batch: bool = False,\n",
        "                 batch_size: int = None,\n",
        "                 is_training: bool = False,\n",
        "                 rng = None,\n",
        "                 ):\n",
        "        rng = hk.PRNGSequence(42) if rng is None else rng\n",
        "\n",
        "        # Layer 1\n",
        "        nodes = jax.nn.relu(self.conv1(nodes, senders, receivers))\n",
        "        nodes, senders, receivers, _, batch = self.pool1(nodes, senders, receivers, edges, batch,\n",
        "                                                         create_new_batch, batch_size)\n",
        "        batch_size += 1  # jit-able topk pooling creates a new batch\n",
        "        x1 = jnp.concatenate([gmp(nodes, batch, batch_size), gap(nodes, batch, batch_size)], axis=1)\n",
        "\n",
        "        # Layer 2\n",
        "        nodes = jax.nn.relu(self.conv2(nodes, senders, receivers))\n",
        "        nodes, senders, receivers, _, batch = self.pool2(nodes, senders, receivers, edges, batch,\n",
        "                                                         create_new_batch, batch_size)\n",
        "        batch_size += 1  # jit-able topk pooling creates a new batch\n",
        "        x2 = jnp.concatenate([gmp(nodes, batch, batch_size), gap(nodes, batch, batch_size)], axis=1)\n",
        "\n",
        "        # Layer 3\n",
        "        nodes = jax.nn.relu(self.conv3(nodes, senders, receivers))\n",
        "        nodes, senders, receivers, _, batch = self.pool3(nodes, senders, receivers, edges, batch,\n",
        "                                                         create_new_batch, batch_size)\n",
        "        batch_size += 1  # jit-able topk pooling creates a new batch\n",
        "        x3 = jnp.concatenate([gmp(nodes, batch, batch_size), gap(nodes, batch, batch_size)], axis=1)\n",
        "\n",
        "        # Aggreagete\n",
        "        nodes = x1[:-1] + x2[:-2] + x3[:-3]  # We exclude each of the new batches created by the static topk pooling\n",
        "\n",
        "        ll1 = self.lin1(nodes)\n",
        "        nodes = jax.nn.relu(ll1)\n",
        "        if is_training:\n",
        "            nodes = hk.dropout(next(rng), 0.5, nodes)\n",
        "        nodes = jax.nn.relu(self.lin2(nodes))\n",
        "        nodes = self.lin3(nodes)\n",
        "        return nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aGbnuBhAUZtx"
      },
      "outputs": [],
      "source": [
        "# Define the function that we will transform with Haiku\n",
        "def forward(graph, batch, create_new_batch, batch_size, is_training=False):\n",
        "    nodes, senders, receivers, labels = graph.nodes, graph.senders, graph.receivers, graph.globals\n",
        "    edges = None\n",
        "    module = TopkNetwork()\n",
        "    return module(nodes, senders, receivers, edges, batch, create_new_batch, batch_size, is_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WPZT3VxVjIh"
      },
      "source": [
        "# 4. Training utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNpmTVOeWEfW"
      },
      "source": [
        "First we create the cross-entropy function, weight update function and accuracy. Notice that all the functions are jit-able and depend on the static parameters\n",
        "`create_new_batch` and `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "laJloHx8WCgl"
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit, static_argnums=(3, 4))\n",
        "def prediction_loss(params_n, graph, batch_idx, create_new_batch, batch_size, labels):\n",
        "    logits = network.apply(params_n, graph, batch_idx, create_new_batch, batch_size, True)\n",
        "    logits = jax.nn.log_softmax(logits)\n",
        "    one_hot_labels = jax.nn.one_hot(labels, 2)\n",
        "    log_likelihood = jnp.sum(one_hot_labels * logits)\n",
        "    return -log_likelihood\n",
        "\n",
        "@partial(jax.jit, static_argnums=(4, 5))\n",
        "def update(params, opt_state, graph, batch_idx, create_new_batch, batch_size, labels):\n",
        "    v, g = jax.value_and_grad(prediction_loss)(params, graph, batch_idx, create_new_batch, batch_size, labels)\n",
        "    updates, opt_state = opt_update(g, opt_state)\n",
        "    return optax.apply_updates(params, updates), opt_state, v\n",
        "\n",
        "@jax.jit\n",
        "def accuracy(decoded_nodes, labels):\n",
        "    dec = jnp.argmax(decoded_nodes, axis=1)\n",
        "    return jnp.mean(dec == labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYJhNXpkWKV1"
      },
      "source": [
        "We also define an eval function to evaluate the validation and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mTPxcCUFWOzz"
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit, static_argnums=(2, 3))\n",
        "def network_forward(params_n, dataset, create_new_batch, batch_size):\n",
        "  out = network.apply(params_n, gs, batch_idx, create_new_batch, batch_size, False)\n",
        "  return out\n",
        "\n",
        "def eval(params_n, dataset, create_new_batch, batch_size):\n",
        "    sum_acc = 0\n",
        "    for it, graph_list in enumerate(list_batch(dataset, batch_size)):\n",
        "        num_graphs = len(graph_list)\n",
        "        gs, batch_idx = batch(graph_list)\n",
        "        labels = gs.globals - 1  # Labels are 1 and 2, we want 0 and 1\n",
        "        out = network_forward(params_n, dataset, create_new_batch, num_graphs)\n",
        "        #out = network.apply(params_n, gs, batch_idx, create_new_batch, num_graphs, False)\n",
        "        acc = accuracy(out, labels).item()\n",
        "        sum_acc += acc\n",
        "    return sum_acc / (it + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u785tM4Y1njy"
      },
      "source": [
        "Since we are working with in-memory lists as datasets, we will also create an auxiliary function to batch a list of graphs, obtained from an iterator, into a single graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9upsxX3n178T"
      },
      "outputs": [],
      "source": [
        "# Auxiliary method that will batch graphs from 'ds' iterator of size 'batch_size'\n",
        "def list_batch(dataset_iterator, batch_size):\n",
        "    graphs = []\n",
        "    for g in dataset_iterator:\n",
        "        graphs.append(g)\n",
        "        if len(graphs) == batch_size:\n",
        "            yield graphs\n",
        "            graphs = []\n",
        "    yield graphs  # ds has finished"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7dh-UX119Z9"
      },
      "source": [
        "# 5. Train!\n",
        "\n",
        "We are ready to train out model. First we define the following hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yapivgQl2Jfj"
      },
      "outputs": [],
      "source": [
        "epochs = 200\n",
        "\n",
        "# static params needed for jitable topk\n",
        "create_new_batch = True\n",
        "batch_size = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QmZI0vz2YA1"
      },
      "source": [
        "We now define our optimizer. We will use Adam optimizer with a learning rate of 0.0005."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wRQCt4b22fJI"
      },
      "outputs": [],
      "source": [
        "# Initialize optimizer\n",
        "opt_init, opt_update = optax.adam(0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfz5vyVZ2h68"
      },
      "source": [
        "Finally, we train our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK9GZKjO2kaz",
        "outputId": "6587f702-73ff-4306-e0e3-02f504a42dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0   Loss: 99.5463   Time: 18.9826   Train Accuracy: 0.5093   Val Accuracy: 0.5270   Test Accuracy: 0.5109\n",
            "Epoch: 1   Loss: 47.3293   Time: 0.2889   Train Accuracy: 0.5744   Val Accuracy: 0.4407   Test Accuracy: 0.4776\n",
            "Epoch: 2   Loss: 38.7097   Time: 0.2245   Train Accuracy: 0.6816   Val Accuracy: 0.4936   Test Accuracy: 0.5135\n",
            "Epoch: 3   Loss: 34.1965   Time: 0.2205   Train Accuracy: 0.6980   Val Accuracy: 0.4922   Test Accuracy: 0.5147\n",
            "Epoch: 4   Loss: 31.7194   Time: 0.3035   Train Accuracy: 0.7202   Val Accuracy: 0.4966   Test Accuracy: 0.4942\n",
            "Epoch: 5   Loss: 30.6085   Time: 0.2149   Train Accuracy: 0.7371   Val Accuracy: 0.4784   Test Accuracy: 0.4763\n",
            "Epoch: 6   Loss: 29.8576   Time: 0.2185   Train Accuracy: 0.7547   Val Accuracy: 0.4980   Test Accuracy: 0.5122\n",
            "Epoch: 7   Loss: 29.3556   Time: 0.2838   Train Accuracy: 0.7544   Val Accuracy: 0.5176   Test Accuracy: 0.5122\n",
            "Epoch: 8   Loss: 28.6304   Time: 0.2252   Train Accuracy: 0.7664   Val Accuracy: 0.5176   Test Accuracy: 0.5122\n",
            "Epoch: 9   Loss: 28.2360   Time: 0.2162   Train Accuracy: 0.7744   Val Accuracy: 0.4980   Test Accuracy: 0.5096\n",
            "Epoch: 10   Loss: 28.3885   Time: 0.2186   Train Accuracy: 0.7733   Val Accuracy: 0.5162   Test Accuracy: 0.5109\n",
            "Epoch: 11   Loss: 27.4770   Time: 0.2396   Train Accuracy: 0.7860   Val Accuracy: 0.5162   Test Accuracy: 0.5109\n",
            "Epoch: 12   Loss: 27.0118   Time: 0.2425   Train Accuracy: 0.7827   Val Accuracy: 0.4980   Test Accuracy: 0.5096\n",
            "Epoch: 13   Loss: 26.9021   Time: 0.2161   Train Accuracy: 0.7864   Val Accuracy: 0.5162   Test Accuracy: 0.5109\n",
            "Epoch: 14   Loss: 26.4345   Time: 0.2194   Train Accuracy: 0.7944   Val Accuracy: 0.5162   Test Accuracy: 0.5109\n",
            "Epoch: 15   Loss: 26.2478   Time: 0.2210   Train Accuracy: 0.8000   Val Accuracy: 0.5162   Test Accuracy: 0.5109\n",
            "Epoch: 16   Loss: 25.8555   Time: 0.2278   Train Accuracy: 0.7991   Val Accuracy: 0.4966   Test Accuracy: 0.5109\n",
            "Epoch: 17   Loss: 25.6814   Time: 0.2106   Train Accuracy: 0.8002   Val Accuracy: 0.4980   Test Accuracy: 0.5096\n",
            "Epoch: 18   Loss: 25.4207   Time: 0.2143   Train Accuracy: 0.7973   Val Accuracy: 0.5162   Test Accuracy: 0.5109\n",
            "Epoch: 19   Loss: 25.2070   Time: 0.2183   Train Accuracy: 0.8004   Val Accuracy: 0.5162   Test Accuracy: 0.5109\n",
            "Epoch: 20   Loss: 24.8624   Time: 0.2567   Train Accuracy: 0.7982   Val Accuracy: 0.4951   Test Accuracy: 0.5122\n",
            "Epoch: 21   Loss: 24.7230   Time: 0.2259   Train Accuracy: 0.8051   Val Accuracy: 0.4966   Test Accuracy: 0.5109\n",
            "Epoch: 22   Loss: 24.4560   Time: 0.2119   Train Accuracy: 0.8009   Val Accuracy: 0.4966   Test Accuracy: 0.5109\n",
            "Epoch: 23   Loss: 24.1829   Time: 0.2924   Train Accuracy: 0.8131   Val Accuracy: 0.4951   Test Accuracy: 0.5122\n",
            "Epoch: 24   Loss: 23.9584   Time: 0.2252   Train Accuracy: 0.8109   Val Accuracy: 0.4966   Test Accuracy: 0.5109\n",
            "Epoch: 25   Loss: 24.0150   Time: 0.2107   Train Accuracy: 0.8109   Val Accuracy: 0.4936   Test Accuracy: 0.5109\n",
            "Epoch: 26   Loss: 24.4415   Time: 0.2838   Train Accuracy: 0.8118   Val Accuracy: 0.4951   Test Accuracy: 0.5122\n",
            "Epoch: 27   Loss: 23.7201   Time: 0.2089   Train Accuracy: 0.8131   Val Accuracy: 0.4936   Test Accuracy: 0.5109\n",
            "Epoch: 28   Loss: 23.2868   Time: 0.2249   Train Accuracy: 0.8153   Val Accuracy: 0.5147   Test Accuracy: 0.5122\n",
            "Epoch: 29   Loss: 22.9705   Time: 0.3028   Train Accuracy: 0.8164   Val Accuracy: 0.5132   Test Accuracy: 0.5135\n",
            "Epoch: 30   Loss: 23.3206   Time: 0.2256   Train Accuracy: 0.8087   Val Accuracy: 0.5147   Test Accuracy: 0.5122\n",
            "Epoch: 31   Loss: 22.9891   Time: 0.2223   Train Accuracy: 0.8198   Val Accuracy: 0.4936   Test Accuracy: 0.5109\n",
            "Epoch: 32   Loss: 22.8757   Time: 0.2143   Train Accuracy: 0.8153   Val Accuracy: 0.4936   Test Accuracy: 0.5109\n",
            "Epoch: 33   Loss: 22.5347   Time: 0.2140   Train Accuracy: 0.8164   Val Accuracy: 0.5147   Test Accuracy: 0.5122\n",
            "Epoch: 34   Loss: 22.2230   Time: 0.2161   Train Accuracy: 0.8231   Val Accuracy: 0.4936   Test Accuracy: 0.5109\n",
            "Epoch: 35   Loss: 21.9662   Time: 0.2202   Train Accuracy: 0.8300   Val Accuracy: 0.4936   Test Accuracy: 0.5109\n",
            "Epoch: 36   Loss: 21.7912   Time: 0.2249   Train Accuracy: 0.8311   Val Accuracy: 0.4951   Test Accuracy: 0.4929\n",
            "Epoch: 37   Loss: 21.5298   Time: 0.2243   Train Accuracy: 0.8322   Val Accuracy: 0.4936   Test Accuracy: 0.4942\n",
            "Epoch: 38   Loss: 21.3196   Time: 0.2169   Train Accuracy: 0.8322   Val Accuracy: 0.4936   Test Accuracy: 0.5109\n",
            "Epoch: 39   Loss: 21.1251   Time: 0.2223   Train Accuracy: 0.8356   Val Accuracy: 0.4951   Test Accuracy: 0.4929\n",
            "Epoch: 40   Loss: 20.9305   Time: 0.2284   Train Accuracy: 0.8333   Val Accuracy: 0.4951   Test Accuracy: 0.4929\n",
            "Epoch: 41   Loss: 20.7912   Time: 0.2212   Train Accuracy: 0.8389   Val Accuracy: 0.4951   Test Accuracy: 0.4929\n",
            "Epoch: 42   Loss: 20.4718   Time: 0.2256   Train Accuracy: 0.8400   Val Accuracy: 0.4951   Test Accuracy: 0.4929\n",
            "Epoch: 43   Loss: 20.2496   Time: 0.2227   Train Accuracy: 0.8456   Val Accuracy: 0.4951   Test Accuracy: 0.4929\n",
            "Epoch: 44   Loss: 20.0722   Time: 0.2287   Train Accuracy: 0.8456   Val Accuracy: 0.4755   Test Accuracy: 0.4763\n",
            "Epoch: 45   Loss: 19.9307   Time: 0.2985   Train Accuracy: 0.8444   Val Accuracy: 0.4936   Test Accuracy: 0.4942\n",
            "Epoch: 46   Loss: 19.6345   Time: 0.2123   Train Accuracy: 0.8478   Val Accuracy: 0.4770   Test Accuracy: 0.4942\n",
            "Epoch: 47   Loss: 19.4594   Time: 0.2127   Train Accuracy: 0.8522   Val Accuracy: 0.4936   Test Accuracy: 0.4942\n",
            "Epoch: 48   Loss: 19.2030   Time: 0.2960   Train Accuracy: 0.8511   Val Accuracy: 0.4755   Test Accuracy: 0.4763\n",
            "Epoch: 49   Loss: 19.1475   Time: 0.2227   Train Accuracy: 0.8500   Val Accuracy: 0.4951   Test Accuracy: 0.4929\n",
            "Epoch: 50   Loss: 18.8624   Time: 0.2318   Train Accuracy: 0.8567   Val Accuracy: 0.4951   Test Accuracy: 0.4929\n",
            "Epoch: 51   Loss: 18.5977   Time: 0.2592   Train Accuracy: 0.8589   Val Accuracy: 0.4770   Test Accuracy: 0.4942\n",
            "Epoch: 52   Loss: 18.4076   Time: 0.2140   Train Accuracy: 0.8622   Val Accuracy: 0.4770   Test Accuracy: 0.4942\n",
            "Epoch: 53   Loss: 18.1930   Time: 0.2226   Train Accuracy: 0.8667   Val Accuracy: 0.4951   Test Accuracy: 0.4929\n",
            "Epoch: 54   Loss: 18.1085   Time: 0.2241   Train Accuracy: 0.8656   Val Accuracy: 0.4951   Test Accuracy: 0.4929\n",
            "Epoch: 55   Loss: 17.8758   Time: 0.2191   Train Accuracy: 0.8711   Val Accuracy: 0.4770   Test Accuracy: 0.4942\n",
            "Epoch: 56   Loss: 17.6565   Time: 0.2248   Train Accuracy: 0.8689   Val Accuracy: 0.4770   Test Accuracy: 0.4942\n",
            "Epoch: 57   Loss: 17.4760   Time: 0.2185   Train Accuracy: 0.8767   Val Accuracy: 0.4755   Test Accuracy: 0.4763\n",
            "Epoch: 58   Loss: 17.4179   Time: 0.2226   Train Accuracy: 0.8689   Val Accuracy: 0.4784   Test Accuracy: 0.4904\n",
            "Epoch: 59   Loss: 17.5043   Time: 0.2239   Train Accuracy: 0.8767   Val Accuracy: 0.4755   Test Accuracy: 0.4763\n",
            "Epoch: 60   Loss: 17.1655   Time: 0.2195   Train Accuracy: 0.8678   Val Accuracy: 0.4770   Test Accuracy: 0.4750\n",
            "Epoch: 61   Loss: 16.7826   Time: 0.2171   Train Accuracy: 0.8744   Val Accuracy: 0.4784   Test Accuracy: 0.4929\n",
            "Epoch: 62   Loss: 16.4878   Time: 0.2297   Train Accuracy: 0.8800   Val Accuracy: 0.4770   Test Accuracy: 0.4750\n",
            "Epoch: 63   Loss: 16.3541   Time: 0.2269   Train Accuracy: 0.8811   Val Accuracy: 0.4784   Test Accuracy: 0.4929\n",
            "Epoch: 64   Loss: 16.1450   Time: 0.2149   Train Accuracy: 0.8811   Val Accuracy: 0.4784   Test Accuracy: 0.4929\n",
            "Epoch: 65   Loss: 15.8650   Time: 0.2141   Train Accuracy: 0.8867   Val Accuracy: 0.4784   Test Accuracy: 0.4929\n",
            "Epoch: 66   Loss: 15.7899   Time: 0.2124   Train Accuracy: 0.8844   Val Accuracy: 0.4799   Test Accuracy: 0.4917\n",
            "Epoch: 67   Loss: 15.5585   Time: 0.2945   Train Accuracy: 0.8889   Val Accuracy: 0.4784   Test Accuracy: 0.4929\n",
            "Epoch: 68   Loss: 15.5956   Time: 0.2263   Train Accuracy: 0.8878   Val Accuracy: 0.4784   Test Accuracy: 0.4737\n",
            "Epoch: 69   Loss: 15.4688   Time: 0.2165   Train Accuracy: 0.8889   Val Accuracy: 0.4799   Test Accuracy: 0.4917\n",
            "Epoch: 70   Loss: 15.5117   Time: 0.2872   Train Accuracy: 0.8922   Val Accuracy: 0.4574   Test Accuracy: 0.4583\n",
            "Epoch: 71   Loss: 15.3870   Time: 0.2177   Train Accuracy: 0.8856   Val Accuracy: 0.4603   Test Accuracy: 0.4917\n",
            "Epoch: 72   Loss: 15.2371   Time: 0.2174   Train Accuracy: 0.8900   Val Accuracy: 0.4574   Test Accuracy: 0.4583\n",
            "Epoch: 73   Loss: 15.0784   Time: 0.2958   Train Accuracy: 0.8878   Val Accuracy: 0.4603   Test Accuracy: 0.4917\n",
            "Epoch: 74   Loss: 35.8670   Time: 0.2162   Train Accuracy: 0.8778   Val Accuracy: 0.4436   Test Accuracy: 0.4583\n",
            "Epoch: 75   Loss: 16.8689   Time: 0.2259   Train Accuracy: 0.8844   Val Accuracy: 0.4755   Test Accuracy: 0.4737\n",
            "Epoch: 76   Loss: 15.6668   Time: 0.2848   Train Accuracy: 0.8789   Val Accuracy: 0.4603   Test Accuracy: 0.4724\n",
            "Epoch: 77   Loss: 14.8525   Time: 0.2246   Train Accuracy: 0.8922   Val Accuracy: 0.4770   Test Accuracy: 0.4750\n",
            "Epoch: 78   Loss: 14.2697   Time: 0.2193   Train Accuracy: 0.8989   Val Accuracy: 0.4574   Test Accuracy: 0.4558\n",
            "Epoch: 79   Loss: 13.9838   Time: 0.2208   Train Accuracy: 0.8978   Val Accuracy: 0.4784   Test Accuracy: 0.4737\n",
            "Epoch: 80   Loss: 13.7784   Time: 0.2286   Train Accuracy: 0.9022   Val Accuracy: 0.4603   Test Accuracy: 0.4558\n",
            "Epoch: 81   Loss: 13.6331   Time: 0.2279   Train Accuracy: 0.9100   Val Accuracy: 0.4588   Test Accuracy: 0.4545\n",
            "Epoch: 82   Loss: 13.5758   Time: 0.2205   Train Accuracy: 0.9022   Val Accuracy: 0.4784   Test Accuracy: 0.4737\n",
            "Epoch: 83   Loss: 13.1985   Time: 0.2105   Train Accuracy: 0.9078   Val Accuracy: 0.4603   Test Accuracy: 0.4776\n",
            "Epoch: 84   Loss: 13.1708   Time: 0.2139   Train Accuracy: 0.9100   Val Accuracy: 0.4784   Test Accuracy: 0.4737\n",
            "Epoch: 85   Loss: 13.0337   Time: 0.2156   Train Accuracy: 0.9133   Val Accuracy: 0.4618   Test Accuracy: 0.4763\n",
            "Epoch: 86   Loss: 13.0310   Time: 0.2869   Train Accuracy: 0.9044   Val Accuracy: 0.4799   Test Accuracy: 0.4917\n",
            "Epoch: 87   Loss: 12.6369   Time: 0.2149   Train Accuracy: 0.9111   Val Accuracy: 0.4603   Test Accuracy: 0.4750\n",
            "Epoch: 88   Loss: 12.8521   Time: 0.2269   Train Accuracy: 0.9067   Val Accuracy: 0.4603   Test Accuracy: 0.4558\n",
            "Epoch: 89   Loss: 12.5902   Time: 0.3173   Train Accuracy: 0.9178   Val Accuracy: 0.4618   Test Accuracy: 0.4596\n",
            "Epoch: 90   Loss: 12.1517   Time: 0.2308   Train Accuracy: 0.9122   Val Accuracy: 0.4603   Test Accuracy: 0.4724\n",
            "Epoch: 91   Loss: 12.1419   Time: 0.2202   Train Accuracy: 0.9167   Val Accuracy: 0.4588   Test Accuracy: 0.4545\n",
            "Epoch: 92   Loss: 12.0374   Time: 0.2685   Train Accuracy: 0.9211   Val Accuracy: 0.4603   Test Accuracy: 0.4724\n",
            "Epoch: 93   Loss: 18.5895   Time: 0.2151   Train Accuracy: 0.8751   Val Accuracy: 0.4980   Test Accuracy: 0.5122\n",
            "Epoch: 94   Loss: 18.4823   Time: 0.2215   Train Accuracy: 0.8642   Val Accuracy: 0.4618   Test Accuracy: 0.4545\n",
            "Epoch: 95   Loss: 17.0479   Time: 0.2890   Train Accuracy: 0.8753   Val Accuracy: 0.4755   Test Accuracy: 0.4545\n",
            "Epoch: 96   Loss: 15.3775   Time: 0.2387   Train Accuracy: 0.8911   Val Accuracy: 0.4799   Test Accuracy: 0.4724\n",
            "Epoch: 97   Loss: 14.4095   Time: 0.2208   Train Accuracy: 0.8956   Val Accuracy: 0.4799   Test Accuracy: 0.4917\n",
            "Epoch: 98   Loss: 13.9946   Time: 0.2128   Train Accuracy: 0.9100   Val Accuracy: 0.4799   Test Accuracy: 0.4724\n",
            "Epoch: 99   Loss: 13.2855   Time: 0.2292   Train Accuracy: 0.9111   Val Accuracy: 0.4784   Test Accuracy: 0.4737\n",
            "Epoch: 100   Loss: 13.0474   Time: 0.2133   Train Accuracy: 0.9156   Val Accuracy: 0.4784   Test Accuracy: 0.4737\n",
            "Epoch: 101   Loss: 12.9028   Time: 0.2214   Train Accuracy: 0.9144   Val Accuracy: 0.4799   Test Accuracy: 0.4724\n",
            "Epoch: 102   Loss: 12.5814   Time: 0.2172   Train Accuracy: 0.9211   Val Accuracy: 0.4799   Test Accuracy: 0.4917\n",
            "Epoch: 103   Loss: 12.2789   Time: 0.2225   Train Accuracy: 0.9244   Val Accuracy: 0.4814   Test Accuracy: 0.4929\n",
            "Epoch: 104   Loss: 12.0967   Time: 0.2264   Train Accuracy: 0.9200   Val Accuracy: 0.4632   Test Accuracy: 0.4942\n",
            "Epoch: 105   Loss: 11.7020   Time: 0.2176   Train Accuracy: 0.9287   Val Accuracy: 0.4603   Test Accuracy: 0.4750\n",
            "Epoch: 106   Loss: 11.2609   Time: 0.2213   Train Accuracy: 0.9278   Val Accuracy: 0.4603   Test Accuracy: 0.4750\n",
            "Epoch: 107   Loss: 10.9226   Time: 0.2161   Train Accuracy: 0.9311   Val Accuracy: 0.4603   Test Accuracy: 0.4724\n",
            "Epoch: 108   Loss: 10.8055   Time: 0.3048   Train Accuracy: 0.9322   Val Accuracy: 0.4603   Test Accuracy: 0.4750\n",
            "Epoch: 109   Loss: 10.6429   Time: 0.2242   Train Accuracy: 0.9356   Val Accuracy: 0.4603   Test Accuracy: 0.4750\n",
            "Epoch: 110   Loss: 10.4130   Time: 0.2136   Train Accuracy: 0.9300   Val Accuracy: 0.4603   Test Accuracy: 0.4750\n",
            "Epoch: 111   Loss: 10.2925   Time: 0.2984   Train Accuracy: 0.9322   Val Accuracy: 0.4618   Test Accuracy: 0.4737\n",
            "Epoch: 112   Loss: 10.2195   Time: 0.2138   Train Accuracy: 0.9389   Val Accuracy: 0.4618   Test Accuracy: 0.4737\n",
            "Epoch: 113   Loss: 10.1622   Time: 0.2155   Train Accuracy: 0.9367   Val Accuracy: 0.4603   Test Accuracy: 0.4558\n",
            "Epoch: 114   Loss: 10.0469   Time: 0.2890   Train Accuracy: 0.9344   Val Accuracy: 0.4618   Test Accuracy: 0.4737\n",
            "Epoch: 115   Loss: 9.7886   Time: 0.2175   Train Accuracy: 0.9389   Val Accuracy: 0.4618   Test Accuracy: 0.4737\n",
            "Epoch: 116   Loss: 9.7911   Time: 0.2217   Train Accuracy: 0.9398   Val Accuracy: 0.4588   Test Accuracy: 0.4545\n",
            "Epoch: 117   Loss: 9.5982   Time: 0.2189   Train Accuracy: 0.9411   Val Accuracy: 0.4436   Test Accuracy: 0.4750\n",
            "Epoch: 118   Loss: 9.5530   Time: 0.2223   Train Accuracy: 0.9387   Val Accuracy: 0.4603   Test Accuracy: 0.4724\n",
            "Epoch: 119   Loss: 9.3391   Time: 0.2261   Train Accuracy: 0.9422   Val Accuracy: 0.4603   Test Accuracy: 0.4724\n",
            "Epoch: 120   Loss: 9.3779   Time: 0.2160   Train Accuracy: 0.9433   Val Accuracy: 0.4422   Test Accuracy: 0.4571\n",
            "Epoch: 121   Loss: 9.9151   Time: 0.2327   Train Accuracy: 0.9422   Val Accuracy: 0.4422   Test Accuracy: 0.4571\n",
            "Epoch: 122   Loss: 9.5645   Time: 0.2115   Train Accuracy: 0.9433   Val Accuracy: 0.4603   Test Accuracy: 0.4583\n",
            "Epoch: 123   Loss: 9.2108   Time: 0.2178   Train Accuracy: 0.9411   Val Accuracy: 0.4422   Test Accuracy: 0.4571\n",
            "Epoch: 124   Loss: 9.0484   Time: 0.2249   Train Accuracy: 0.9456   Val Accuracy: 0.4603   Test Accuracy: 0.4724\n",
            "Epoch: 125   Loss: 8.8355   Time: 0.2267   Train Accuracy: 0.9467   Val Accuracy: 0.4422   Test Accuracy: 0.4571\n",
            "Epoch: 126   Loss: 8.8729   Time: 0.2333   Train Accuracy: 0.9489   Val Accuracy: 0.4436   Test Accuracy: 0.4750\n",
            "Epoch: 127   Loss: 8.5045   Time: 0.2783   Train Accuracy: 0.9533   Val Accuracy: 0.4422   Test Accuracy: 0.4737\n",
            "Epoch: 128   Loss: 8.5845   Time: 0.2218   Train Accuracy: 0.9544   Val Accuracy: 0.4407   Test Accuracy: 0.4558\n",
            "Epoch: 129   Loss: 8.6988   Time: 0.2186   Train Accuracy: 0.9522   Val Accuracy: 0.4407   Test Accuracy: 0.4558\n",
            "Epoch: 130   Loss: 8.4823   Time: 0.2886   Train Accuracy: 0.9533   Val Accuracy: 0.4422   Test Accuracy: 0.4571\n",
            "Epoch: 131   Loss: 8.6869   Time: 0.2185   Train Accuracy: 0.9522   Val Accuracy: 0.4451   Test Accuracy: 0.4737\n",
            "Epoch: 132   Loss: 8.6287   Time: 0.2240   Train Accuracy: 0.9489   Val Accuracy: 0.4407   Test Accuracy: 0.4558\n",
            "Epoch: 133   Loss: 8.5318   Time: 0.2935   Train Accuracy: 0.9533   Val Accuracy: 0.4451   Test Accuracy: 0.4737\n",
            "Epoch: 134   Loss: 8.5317   Time: 0.2185   Train Accuracy: 0.9478   Val Accuracy: 0.4422   Test Accuracy: 0.4737\n",
            "Epoch: 135   Loss: 8.7565   Time: 0.2229   Train Accuracy: 0.9422   Val Accuracy: 0.4422   Test Accuracy: 0.4571\n",
            "Epoch: 136   Loss: 8.7009   Time: 0.2111   Train Accuracy: 0.9444   Val Accuracy: 0.4422   Test Accuracy: 0.4545\n",
            "Epoch: 137   Loss: 9.2047   Time: 0.2187   Train Accuracy: 0.9389   Val Accuracy: 0.4451   Test Accuracy: 0.4904\n",
            "Epoch: 138   Loss: 9.7005   Time: 0.2120   Train Accuracy: 0.9333   Val Accuracy: 0.4451   Test Accuracy: 0.4904\n",
            "Epoch: 139   Loss: 10.0455   Time: 0.2328   Train Accuracy: 0.9344   Val Accuracy: 0.4574   Test Accuracy: 0.4558\n",
            "Epoch: 140   Loss: 9.4845   Time: 0.2162   Train Accuracy: 0.9333   Val Accuracy: 0.4618   Test Accuracy: 0.4904\n",
            "Epoch: 141   Loss: 10.7123   Time: 0.2158   Train Accuracy: 0.9189   Val Accuracy: 0.4618   Test Accuracy: 0.4737\n",
            "Epoch: 142   Loss: 11.8313   Time: 0.2228   Train Accuracy: 0.9167   Val Accuracy: 0.4436   Test Accuracy: 0.4891\n",
            "Epoch: 143   Loss: 12.8254   Time: 0.2124   Train Accuracy: 0.9056   Val Accuracy: 0.4618   Test Accuracy: 0.4904\n",
            "Epoch: 144   Loss: 13.0635   Time: 0.2135   Train Accuracy: 0.8976   Val Accuracy: 0.4814   Test Accuracy: 0.5071\n",
            "Epoch: 145   Loss: 16.4447   Time: 0.2212   Train Accuracy: 0.8800   Val Accuracy: 0.5176   Test Accuracy: 0.5237\n",
            "Epoch: 146   Loss: 16.0243   Time: 0.2912   Train Accuracy: 0.8876   Val Accuracy: 0.5176   Test Accuracy: 0.5404\n",
            "Epoch: 147   Loss: 17.2278   Time: 0.2209   Train Accuracy: 0.8696   Val Accuracy: 0.4966   Test Accuracy: 0.5609\n",
            "Epoch: 148   Loss: 24.8027   Time: 0.2264   Train Accuracy: 0.8329   Val Accuracy: 0.4966   Test Accuracy: 0.5609\n",
            "Epoch: 149   Loss: 25.5125   Time: 0.2936   Train Accuracy: 0.8376   Val Accuracy: 0.4618   Test Accuracy: 0.4763\n",
            "Epoch: 150   Loss: 15.8528   Time: 0.2465   Train Accuracy: 0.8878   Val Accuracy: 0.4618   Test Accuracy: 0.4737\n",
            "Epoch: 151   Loss: 10.6721   Time: 0.2264   Train Accuracy: 0.9187   Val Accuracy: 0.4632   Test Accuracy: 0.4942\n",
            "Epoch: 152   Loss: 8.3733   Time: 0.2877   Train Accuracy: 0.9464   Val Accuracy: 0.4618   Test Accuracy: 0.4712\n",
            "Epoch: 153   Loss: 9.7761   Time: 0.2224   Train Accuracy: 0.9378   Val Accuracy: 0.4618   Test Accuracy: 0.4904\n",
            "Epoch: 154   Loss: 9.3087   Time: 0.2174   Train Accuracy: 0.9411   Val Accuracy: 0.4618   Test Accuracy: 0.4712\n",
            "Epoch: 155   Loss: 8.5563   Time: 0.2209   Train Accuracy: 0.9500   Val Accuracy: 0.4618   Test Accuracy: 0.4737\n",
            "Epoch: 156   Loss: 7.7768   Time: 0.2181   Train Accuracy: 0.9567   Val Accuracy: 0.4618   Test Accuracy: 0.4904\n",
            "Epoch: 157   Loss: 7.7736   Time: 0.2225   Train Accuracy: 0.9600   Val Accuracy: 0.4451   Test Accuracy: 0.4904\n",
            "Epoch: 158   Loss: 7.1908   Time: 0.2250   Train Accuracy: 0.9611   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 159   Loss: 7.0070   Time: 0.2217   Train Accuracy: 0.9633   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 160   Loss: 6.6262   Time: 0.2210   Train Accuracy: 0.9678   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 161   Loss: 6.4273   Time: 0.2209   Train Accuracy: 0.9722   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 162   Loss: 6.2036   Time: 0.2202   Train Accuracy: 0.9733   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 163   Loss: 6.1140   Time: 0.2238   Train Accuracy: 0.9722   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 164   Loss: 5.9702   Time: 0.2156   Train Accuracy: 0.9744   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 165   Loss: 5.7982   Time: 0.2185   Train Accuracy: 0.9744   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 166   Loss: 5.6537   Time: 0.2307   Train Accuracy: 0.9756   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 167   Loss: 5.5234   Time: 0.2124   Train Accuracy: 0.9767   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 168   Loss: 5.4582   Time: 0.3131   Train Accuracy: 0.9767   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 169   Loss: 5.4109   Time: 0.2216   Train Accuracy: 0.9778   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 170   Loss: 5.2757   Time: 0.2166   Train Accuracy: 0.9789   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 171   Loss: 5.1233   Time: 0.2878   Train Accuracy: 0.9800   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 172   Loss: 5.0213   Time: 0.2212   Train Accuracy: 0.9811   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 173   Loss: 4.9439   Time: 0.2510   Train Accuracy: 0.9800   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 174   Loss: 4.8470   Time: 0.2790   Train Accuracy: 0.9811   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 175   Loss: 4.7506   Time: 0.2221   Train Accuracy: 0.9811   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 176   Loss: 4.6970   Time: 0.2263   Train Accuracy: 0.9811   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 177   Loss: 4.6419   Time: 0.2172   Train Accuracy: 0.9833   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 178   Loss: 4.5124   Time: 0.2125   Train Accuracy: 0.9833   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 179   Loss: 4.4566   Time: 0.2311   Train Accuracy: 0.9833   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 180   Loss: 4.3641   Time: 0.2163   Train Accuracy: 0.9833   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 181   Loss: 4.2977   Time: 0.2205   Train Accuracy: 0.9856   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 182   Loss: 4.2127   Time: 0.2184   Train Accuracy: 0.9844   Val Accuracy: 0.4632   Test Accuracy: 0.4917\n",
            "Epoch: 183   Loss: 4.1447   Time: 0.2206   Train Accuracy: 0.9844   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 184   Loss: 4.0692   Time: 0.2230   Train Accuracy: 0.9844   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 185   Loss: 4.1061   Time: 0.2147   Train Accuracy: 0.9844   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 186   Loss: 4.0087   Time: 0.2210   Train Accuracy: 0.9856   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 187   Loss: 3.9262   Time: 0.2970   Train Accuracy: 0.9844   Val Accuracy: 0.4451   Test Accuracy: 0.4904\n",
            "Epoch: 188   Loss: 3.8549   Time: 0.2274   Train Accuracy: 0.9856   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 189   Loss: 3.8571   Time: 0.2189   Train Accuracy: 0.9900   Val Accuracy: 0.4436   Test Accuracy: 0.4724\n",
            "Epoch: 190   Loss: 3.7859   Time: 0.2726   Train Accuracy: 0.9867   Val Accuracy: 0.4436   Test Accuracy: 0.4724\n",
            "Epoch: 191   Loss: 3.7112   Time: 0.2314   Train Accuracy: 0.9878   Val Accuracy: 0.4436   Test Accuracy: 0.4917\n",
            "Epoch: 192   Loss: 3.6653   Time: 0.2256   Train Accuracy: 0.9856   Val Accuracy: 0.4451   Test Accuracy: 0.4904\n",
            "Epoch: 193   Loss: 3.5970   Time: 0.3029   Train Accuracy: 0.9867   Val Accuracy: 0.4451   Test Accuracy: 0.4904\n",
            "Epoch: 194   Loss: 3.5459   Time: 0.2147   Train Accuracy: 0.9878   Val Accuracy: 0.4451   Test Accuracy: 0.4904\n",
            "Epoch: 195   Loss: 3.4336   Time: 0.2253   Train Accuracy: 0.9889   Val Accuracy: 0.4436   Test Accuracy: 0.4724\n",
            "Epoch: 196   Loss: 3.4201   Time: 0.2290   Train Accuracy: 0.9867   Val Accuracy: 0.4436   Test Accuracy: 0.4724\n",
            "Epoch: 197   Loss: 3.3304   Time: 0.2148   Train Accuracy: 0.9900   Val Accuracy: 0.4436   Test Accuracy: 0.4724\n",
            "Epoch: 198   Loss: 3.2397   Time: 0.2109   Train Accuracy: 0.9900   Val Accuracy: 0.4436   Test Accuracy: 0.4724\n",
            "Epoch: 199   Loss: 3.2126   Time: 0.2194   Train Accuracy: 0.9922   Val Accuracy: 0.4436   Test Accuracy: 0.4724\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "    sum_acc = 0\n",
        "    sum_loss = 0\n",
        "    for it, graph_list in enumerate(list_batch(train_dataset, batch_size)):\n",
        "        start = time.time()\n",
        "        gs, batch_idx = batch(graph_list)\n",
        "        num_graphs = len(graph_list)\n",
        "        labels = gs.globals - 1  # We subtract 1 to have labels in the range 0, 1\n",
        "        if epoch == 0 and it == 0:\n",
        "            # Initialize the model by performing a forward run with the first batch\n",
        "            network = hk.without_apply_rng(hk.transform(forward))\n",
        "            params_n = network.init(jax.random.PRNGKey(42), gs, batch_idx, create_new_batch, num_graphs, True)\n",
        "            opt_state = opt_init(params_n)\n",
        "        else:\n",
        "            out = network.apply(params_n, gs, batch_idx, create_new_batch, num_graphs, True)\n",
        "            acc = accuracy(out, labels).item()\n",
        "            params_n, opt_state, loss = update(params_n, opt_state, gs, batch_idx, create_new_batch, num_graphs, labels)\n",
        "            sum_acc += acc\n",
        "            sum_loss += loss.item()\n",
        "        end = time.time()\n",
        "\n",
        "    val_acc = eval(params_n, val_dataset, create_new_batch, batch_size)\n",
        "    test_acc = eval(params_n, test_dataset, create_new_batch, batch_size)\n",
        "\n",
        "    print(f\"Epoch: {epoch}   Loss: {sum_loss / (it + 1):.4f}   \"\n",
        "          f\"Time: {end - start:.4f}   \"\n",
        "          f\"Train Accuracy: {sum_acc / (it + 1):.4f}   \"\n",
        "          f\"Val Accuracy: {val_acc:.4f}   \"\n",
        "          f\"Test Accuracy: {test_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}