{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lla34xIY64Hh"
      },
      "source": [
        "# Top-k pooling and graph convolution trained on PROTEINS dataset with Haiku Geometric\n",
        "\n",
        "This notebook contains an example on how to use [Haiku Geometric](https://github.com/alexOarga/haiku-geometric) to create graph convolutional networks, graph pooling layers and train them on the PROTEINS dataset.\n",
        "\n",
        "[Haiku Geometric](https://github.com/alexOarga/haiku-geometric) is a graph neural network library built for [JAX](https://github.com/google/jax) + [Haiku](https://github.com/deepmind/dm-haiku).\n",
        "\n",
        "If wou want to know more about Haiku Geometric, please visit the [documentation](https://haiku-geometric.readthedocs.io/en/latest/).\n",
        "You can find there a more detailed explanation of the library and how to use it as well as the API reference.\n",
        "\n",
        "If you want to see other examples on how to use Haiku Geometric to build other\n",
        "graph neural networks, check out the [examples](https://haiku-geometric.readthedocs.io/en/latest/examples.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XxejpLi73VF"
      },
      "source": [
        "# 1. Install and import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n8AqiU0piCo",
        "outputId": "773cc4e0-7f1f-4601-ad7c-3050bc229b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax) (0.1.7)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.14)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.14+cuda11.cudnn86)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from optax) (1.23.5)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax) (4.5.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax) (1.11.2)\n",
            "Collecting git+https://github.com/alexOarga/haiku-geometric.git\n",
            "  Cloning https://github.com/alexOarga/haiku-geometric.git to /tmp/pip-req-build-z_m5gsyp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/alexOarga/haiku-geometric.git /tmp/pip-req-build-z_m5gsyp\n",
            "  Resolved https://github.com/alexOarga/haiku-geometric.git to commit 9683a6160798852d7dc3a6cc2638924dc3665ac6\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from haiku-geometric==0.0.5) (0.4.14)\n",
            "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.10/dist-packages (from haiku-geometric==0.0.5) (0.0.10)\n",
            "Requirement already satisfied: jraph in /usr/local/lib/python3.10/dist-packages (from haiku-geometric==0.0.5) (0.0.6.dev0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from haiku-geometric==0.0.5) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haiku-geometric==0.0.5) (4.66.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from haiku-geometric==0.0.5) (0.56.4)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->haiku-geometric==0.0.5) (1.4.0)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->haiku-geometric==0.0.5) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->haiku-geometric==0.0.5) (1.23.5)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from dm-haiku->haiku-geometric==0.0.5) (0.9.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->haiku-geometric==0.0.5) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->haiku-geometric==0.0.5) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax->haiku-geometric==0.0.5) (1.11.2)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from jraph->haiku-geometric==0.0.5) (0.4.14+cuda11.cudnn86)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->haiku-geometric==0.0.5) (0.39.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install optax\n",
        "!pip install git+https://github.com/alexOarga/haiku-geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "At7GEmL37y0v"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import haiku as hk\n",
        "import jax\n",
        "import time\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "from functools import partial\n",
        "from random import shuffle\n",
        "\n",
        "from haiku_geometric.utils import batch, pad_graph\n",
        "from haiku_geometric.nn import GraphConv, TopKPooling\n",
        "from haiku_geometric.nn.pool import global_mean_pool, global_max_pool\n",
        "from haiku_geometric.datasets import TUDataset\n",
        "\n",
        "gap = global_mean_pool\n",
        "gmp = global_max_pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdTvvCpV8NjW"
      },
      "source": [
        "# 2. Load and prepare dataset\n",
        "\n",
        "\n",
        "Here we will load the PROTEINS dataset from the [TU Dortmund](https://chrsmrrs.github.io/datasets/docs/datasets/). In this notebook we will simply use lists as in-memory datasets, but I encourage you to create your own custom data loader.\n",
        "\n",
        "First, we load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MxM2eQqe8QWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd574ceb-1b83-4451-f85c-88caa4096f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using existing file PROTEINS.zip\n"
          ]
        }
      ],
      "source": [
        "dataset = 'PROTEINS'\n",
        "folder = '.'\n",
        "\n",
        "tu_dataset = TUDataset(dataset, folder, use_node_attr=True)\n",
        "tu_dataset_len = int(len(tu_dataset.data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tsisv4SG_uew",
        "outputId": "e7c9a6d1-f8b3-4424-fdee-212242bac6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of graphs : 1113\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of graphs :\", tu_dataset_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTKkMvlR_aIF"
      },
      "source": [
        "We now shuffle the dataset and split the dataset into train, validation and test using 80%, 10% and 10% respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0qtCWJM98uB-"
      },
      "outputs": [],
      "source": [
        "for i in range(tu_dataset_len):\n",
        "    # ⚠️ Notice: We store each graph label in the 'globals' attribute\n",
        "    tu_dataset.data[i] = tu_dataset.data[i]._replace(globals=tu_dataset.y[i].reshape(1, ))\n",
        "\n",
        "# We shuffle the dataset randomly\n",
        "shuffle(tu_dataset.data)\n",
        "\n",
        "# Prepare the train-val-test splits\n",
        "train_size = int(tu_dataset_len * 0.8)\n",
        "val_size = int(tu_dataset_len * 0.1)\n",
        "test_size = int(tu_dataset_len - train_size - val_size)\n",
        "splits_range = {\n",
        "    'train': (0, train_size),\n",
        "    'val': (train_size, train_size + val_size),\n",
        "    'test': (train_size + val_size, tu_dataset_len)\n",
        "}\n",
        "\n",
        "train_dataset = tu_dataset.data[slice(*splits_range['train'])]\n",
        "val_dataset = tu_dataset.data[slice(*splits_range['val'])]\n",
        "test_dataset = tu_dataset.data[slice(*splits_range['test'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BykR2pvMAqdr"
      },
      "source": [
        "Let us inspect a couple graphs from the train split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d0yM9WuAu2N",
        "outputId": "e86a4c55-0276-4dd7-e22c-cdb5862f2709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph 0: Number of nodes: [146]\n",
            "Graph 0: Number of edges: [482]\n",
            "Graph 0: Nodes features size: 4\n",
            "Graph 0: Label: [1]\n",
            "    \n",
            "Graph 1: Number of nodes: [11]\n",
            "Graph 1: Number of edges: [38]\n",
            "Graph 1: Nodes features size: 4\n",
            "Graph 1: Label: [1]\n",
            "    \n",
            "Graph 2: Number of nodes: [6]\n",
            "Graph 2: Number of edges: [24]\n",
            "Graph 2: Nodes features size: 4\n",
            "Graph 2: Label: [2]\n",
            "    \n",
            "    ...\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "  graph = train_dataset[i]\n",
        "  print(f\"Graph {i}: Number of nodes:\", graph.n_node)\n",
        "  print(f\"Graph {i}: Number of edges:\", graph.n_edge)\n",
        "  print(f\"Graph {i}: Nodes features size:\", graph.nodes.shape[-1])\n",
        "  print(f\"Graph {i}: Label:\", graph.globals)\n",
        "  print(\"    \")\n",
        "print(\"    ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdrFXTzEBpuY"
      },
      "source": [
        "# 3. Define the model\n",
        "\n",
        "We are ready to define our model. First, we will apply a [GraphConv](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.conv.GraphConv) layer and then a [TopKPooling](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.pool.TopKPooling) layer. The top-k pooling layer will reduce the number of nodes in the graph with a reduction factor of 0.8. Finally, we apply both a [global_add_pool](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.pool.global_add_pool) and a [global_mean_pool](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.pool.global_mean_pool) layer and concatenate the output. We repeat this procedure 3 times.\n",
        "\n",
        "## 3.1 JIT-able TopKPooling\n",
        "\n",
        "The [TopKPooling](https://haiku-geometric.readthedocs.io/en/latest/modules/nn.html#haiku_geometric.nn.pool.TopKPooling) layer is not JIT-able by default, because it naturally works on arrays shaped dynamically. However, in order to fully benefit from the speed of JAX, we need to JIT the model, this is, we need to make the layer JIT-able. This can be done by using the `create_new_batch` parameter of the layer during the forward call. If this parameter is set to `True`, the layer does not remove nodes nor edges from the graph, but instead creates a new batch with the nodes that were removed (see image below). Additionally, the edges from the removed edges become self-loops (see image below).\n",
        "\n",
        "![topk-example-image](https://raw.githubusercontent.com/alexOarga/haiku-geometric/331953f617d55126fc57400b53a0af36199a6a3c/docs/source/_static/topk.png)\n",
        "\n",
        "In addition to the `create_new_batch` parameter, to make the layer fully jit-able\n",
        "we will need to provide as static parameter `batch_size`, this is, the numbers of graphs in a batch. This parameter is needed to create the new batch.\n",
        "\n",
        "## 3.2 Model\n",
        "\n",
        "With the above in mind, we are ready to create our model. Notice that we provide the parameters `create_new_batch` and `batch_size` to the TopKPooling layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UvH2cC4CTXdU"
      },
      "outputs": [],
      "source": [
        "class TopkNetwork(hk.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GraphConv(128)\n",
        "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
        "        self.conv2 = GraphConv(128)\n",
        "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
        "        self.conv3 = GraphConv(128)\n",
        "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
        "\n",
        "        self.lin1 = hk.Linear(128)\n",
        "        self.lin2 = hk.Linear(64)\n",
        "        self.lin3 = hk.Linear(2)\n",
        "\n",
        "    def __call__(self,\n",
        "                 nodes: jnp.ndarray = None,\n",
        "                 senders: jnp.ndarray = None,\n",
        "                 receivers: jnp.ndarray = None,\n",
        "                 edges: jnp.ndarray = None,\n",
        "                 batch: jnp.ndarray = None,\n",
        "                 create_new_batch: bool = False,\n",
        "                 batch_size: int = None,\n",
        "                 is_training: bool = False,\n",
        "                 rng = None,\n",
        "                 ):\n",
        "        rng = hk.PRNGSequence(42) if rng is None else rng\n",
        "\n",
        "        # Layer 1\n",
        "        nodes = jax.nn.relu(self.conv1(nodes, senders, receivers))\n",
        "        nodes, senders, receivers, _, batch = self.pool1(nodes, senders, receivers, edges, batch,\n",
        "                                                         create_new_batch, batch_size)\n",
        "        batch_size += 1  # jit-able topk pooling creates a new batch\n",
        "        x1 = jnp.concatenate([gmp(nodes, batch, batch_size), gap(nodes, batch, batch_size)], axis=1)\n",
        "\n",
        "        # Layer 2\n",
        "        nodes = jax.nn.relu(self.conv2(nodes, senders, receivers))\n",
        "        nodes, senders, receivers, _, batch = self.pool2(nodes, senders, receivers, edges, batch,\n",
        "                                                         create_new_batch, batch_size)\n",
        "        batch_size += 1  # jit-able topk pooling creates a new batch\n",
        "        x2 = jnp.concatenate([gmp(nodes, batch, batch_size), gap(nodes, batch, batch_size)], axis=1)\n",
        "\n",
        "        # Layer 3\n",
        "        nodes = jax.nn.relu(self.conv3(nodes, senders, receivers))\n",
        "        nodes, senders, receivers, _, batch = self.pool3(nodes, senders, receivers, edges, batch,\n",
        "                                                         create_new_batch, batch_size)\n",
        "        batch_size += 1  # jit-able topk pooling creates a new batch\n",
        "        x3 = jnp.concatenate([gmp(nodes, batch, batch_size), gap(nodes, batch, batch_size)], axis=1)\n",
        "\n",
        "        # Aggreagete\n",
        "        nodes = x1[:-1] + x2[:-2] + x3[:-3]  # We exclude each of the new batches created by the static topk pooling\n",
        "\n",
        "        ll1 = self.lin1(nodes)\n",
        "        nodes = jax.nn.relu(ll1)\n",
        "        if is_training:\n",
        "            nodes = hk.dropout(next(rng), 0.5, nodes)\n",
        "        nodes = jax.nn.relu(self.lin2(nodes))\n",
        "        nodes = self.lin3(nodes)\n",
        "        return nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aGbnuBhAUZtx"
      },
      "outputs": [],
      "source": [
        "# Define the function that we will transform with Haiku\n",
        "def forward(graph, batch, create_new_batch, batch_size, is_training=False):\n",
        "    nodes, senders, receivers, labels = graph.nodes, graph.senders, graph.receivers, graph.globals\n",
        "    edges = None\n",
        "    module = TopkNetwork()\n",
        "    return module(nodes, senders, receivers, edges, batch, create_new_batch, batch_size, is_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WPZT3VxVjIh"
      },
      "source": [
        "# 4. Training utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNpmTVOeWEfW"
      },
      "source": [
        "First we create the cross-entropy function, weight update function and accuracy. Notice that all the functions are jit-able and depend on the static parameters\n",
        "`create_new_batch` and `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "laJloHx8WCgl"
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit, static_argnums=(3, 4))\n",
        "def prediction_loss(params_n, graph, batch_idx, create_new_batch, batch_size, labels):\n",
        "    logits = network.apply(params_n, graph, batch_idx, create_new_batch, batch_size, True)\n",
        "    logits = jax.nn.log_softmax(logits)\n",
        "    one_hot_labels = jax.nn.one_hot(labels, 2)\n",
        "    log_likelihood = jnp.sum(one_hot_labels * logits)\n",
        "    return -log_likelihood\n",
        "\n",
        "@partial(jax.jit, static_argnums=(4, 5))\n",
        "def update(params, opt_state, graph, batch_idx, create_new_batch, batch_size, labels):\n",
        "    v, g = jax.value_and_grad(prediction_loss)(params, graph, batch_idx, create_new_batch, batch_size, labels)\n",
        "    updates, opt_state = opt_update(g, opt_state)\n",
        "    return optax.apply_updates(params, updates), opt_state, v\n",
        "\n",
        "@jax.jit\n",
        "def accuracy(decoded_nodes, labels):\n",
        "    dec = jnp.argmax(decoded_nodes, axis=1)\n",
        "    return jnp.mean(dec == labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYJhNXpkWKV1"
      },
      "source": [
        "We also define an eval function to evaluate the validation and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mTPxcCUFWOzz"
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit, static_argnums=(3, 4))\n",
        "def network_forward(params_n, graph, batch_idx, create_new_batch, batch_size):\n",
        "  out = network.apply(params_n, graph, batch_idx, create_new_batch, batch_size, False)\n",
        "  return out\n",
        "\n",
        "def eval(params_n, dataset, create_new_batch, batch_size):\n",
        "    sum_acc = 0\n",
        "    for it, graph_list in enumerate(list_batch(dataset, batch_size)):\n",
        "        num_graphs = len(graph_list)\n",
        "        gs, batch_idx = batch(graph_list)\n",
        "        labels = gs.globals - 1  # Labels are 1 and 2, we want 0 and 1\n",
        "        out = network_forward(params_n, gs, batch_idx, create_new_batch, num_graphs)\n",
        "        #out = network.apply(params_n, gs, batch_idx, create_new_batch, num_graphs, False)\n",
        "        acc = accuracy(out, labels).item()\n",
        "        sum_acc += acc\n",
        "    return sum_acc / (it + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u785tM4Y1njy"
      },
      "source": [
        "Since we are working with in-memory lists as datasets, we will also create an auxiliary function to batch a list of graphs, obtained from an iterator, into a single graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9upsxX3n178T"
      },
      "outputs": [],
      "source": [
        "# Auxiliary method that will batch graphs from 'ds' iterator of size 'batch_size'\n",
        "def list_batch(dataset_iterator, batch_size):\n",
        "    graphs = []\n",
        "    for g in dataset_iterator:\n",
        "        graphs.append(g)\n",
        "        if len(graphs) == batch_size:\n",
        "            yield graphs\n",
        "            graphs = []\n",
        "    yield graphs  # ds has finished"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7dh-UX119Z9"
      },
      "source": [
        "# 5. Train!\n",
        "\n",
        "We are ready to train out model. First we define the following hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yapivgQl2Jfj"
      },
      "outputs": [],
      "source": [
        "epochs = 200\n",
        "\n",
        "# static params needed for jitable topk\n",
        "create_new_batch = True\n",
        "batch_size = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QmZI0vz2YA1"
      },
      "source": [
        "We now define our optimizer. We will use Adam optimizer with a learning rate of 0.0005."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wRQCt4b22fJI"
      },
      "outputs": [],
      "source": [
        "# Initialize optimizer\n",
        "opt_init, opt_update = optax.adam(0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfz5vyVZ2h68"
      },
      "source": [
        "Finally, we train our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK9GZKjO2kaz",
        "outputId": "600c273f-b113-4cdd-d457-02ca9dc7d388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0   Loss: 81.7955   Time: 22.3325   Train Accuracy: 0.5593   Val Accuracy: 0.5956   Test Accuracy: 0.5596\n",
            "Epoch: 1   Loss: 52.4527   Time: 0.2491   Train Accuracy: 0.6616   Val Accuracy: 0.6471   Test Accuracy: 0.7199\n",
            "Epoch: 2   Loss: 38.2288   Time: 0.3155   Train Accuracy: 0.6600   Val Accuracy: 0.6485   Test Accuracy: 0.7103\n",
            "Epoch: 3   Loss: 34.9029   Time: 0.2380   Train Accuracy: 0.7138   Val Accuracy: 0.6569   Test Accuracy: 0.7019\n",
            "Epoch: 4   Loss: 32.4224   Time: 0.2307   Train Accuracy: 0.7360   Val Accuracy: 0.6402   Test Accuracy: 0.7282\n",
            "Epoch: 5   Loss: 30.9986   Time: 0.3755   Train Accuracy: 0.7469   Val Accuracy: 0.6583   Test Accuracy: 0.7641\n",
            "Epoch: 6   Loss: 30.1713   Time: 0.2341   Train Accuracy: 0.7493   Val Accuracy: 0.6569   Test Accuracy: 0.7545\n",
            "Epoch: 7   Loss: 29.4465   Time: 0.2237   Train Accuracy: 0.7547   Val Accuracy: 0.6569   Test Accuracy: 0.7545\n",
            "Epoch: 8   Loss: 28.8113   Time: 0.2940   Train Accuracy: 0.7591   Val Accuracy: 0.6569   Test Accuracy: 0.7449\n",
            "Epoch: 9   Loss: 28.2525   Time: 0.2523   Train Accuracy: 0.7569   Val Accuracy: 0.6569   Test Accuracy: 0.7532\n",
            "Epoch: 10   Loss: 28.0061   Time: 0.2451   Train Accuracy: 0.7684   Val Accuracy: 0.6569   Test Accuracy: 0.7532\n",
            "Epoch: 11   Loss: 27.5671   Time: 0.2398   Train Accuracy: 0.7707   Val Accuracy: 0.6569   Test Accuracy: 0.7365\n",
            "Epoch: 12   Loss: 27.6191   Time: 0.2320   Train Accuracy: 0.7740   Val Accuracy: 0.6667   Test Accuracy: 0.7545\n",
            "Epoch: 13   Loss: 26.8343   Time: 0.2295   Train Accuracy: 0.7796   Val Accuracy: 0.6667   Test Accuracy: 0.7628\n",
            "Epoch: 14   Loss: 26.5932   Time: 0.2442   Train Accuracy: 0.7729   Val Accuracy: 0.6667   Test Accuracy: 0.7449\n",
            "Epoch: 15   Loss: 26.3017   Time: 0.2676   Train Accuracy: 0.7740   Val Accuracy: 0.6667   Test Accuracy: 0.7449\n",
            "Epoch: 16   Loss: 25.9291   Time: 0.2577   Train Accuracy: 0.7773   Val Accuracy: 0.6667   Test Accuracy: 0.7545\n",
            "Epoch: 17   Loss: 25.6077   Time: 0.2404   Train Accuracy: 0.7784   Val Accuracy: 0.6667   Test Accuracy: 0.7545\n",
            "Epoch: 18   Loss: 25.3642   Time: 0.3535   Train Accuracy: 0.7787   Val Accuracy: 0.6667   Test Accuracy: 0.7545\n",
            "Epoch: 19   Loss: 25.1654   Time: 0.2423   Train Accuracy: 0.7796   Val Accuracy: 0.6667   Test Accuracy: 0.7545\n",
            "Epoch: 20   Loss: 24.8681   Time: 0.2327   Train Accuracy: 0.7753   Val Accuracy: 0.6667   Test Accuracy: 0.7545\n",
            "Epoch: 21   Loss: 24.6999   Time: 0.2607   Train Accuracy: 0.7856   Val Accuracy: 0.6667   Test Accuracy: 0.7545\n",
            "Epoch: 22   Loss: 24.4435   Time: 0.2338   Train Accuracy: 0.7836   Val Accuracy: 0.6667   Test Accuracy: 0.7545\n",
            "Epoch: 23   Loss: 24.3133   Time: 0.2426   Train Accuracy: 0.7856   Val Accuracy: 0.6667   Test Accuracy: 0.7449\n",
            "Epoch: 24   Loss: 23.9739   Time: 0.2980   Train Accuracy: 0.7869   Val Accuracy: 0.6667   Test Accuracy: 0.7545\n",
            "Epoch: 25   Loss: 23.6943   Time: 0.2365   Train Accuracy: 0.7891   Val Accuracy: 0.6667   Test Accuracy: 0.7449\n",
            "Epoch: 26   Loss: 23.5832   Time: 0.2443   Train Accuracy: 0.7962   Val Accuracy: 0.6667   Test Accuracy: 0.7545\n",
            "Epoch: 27   Loss: 23.4515   Time: 0.3362   Train Accuracy: 0.7951   Val Accuracy: 0.6667   Test Accuracy: 0.7449\n",
            "Epoch: 28   Loss: 23.2915   Time: 0.2307   Train Accuracy: 0.8020   Val Accuracy: 0.6765   Test Accuracy: 0.7545\n",
            "Epoch: 29   Loss: 22.8523   Time: 0.2263   Train Accuracy: 0.8053   Val Accuracy: 0.6681   Test Accuracy: 0.7545\n",
            "Epoch: 30   Loss: 22.5034   Time: 0.3230   Train Accuracy: 0.8031   Val Accuracy: 0.6765   Test Accuracy: 0.7545\n",
            "Epoch: 31   Loss: 22.3021   Time: 0.3290   Train Accuracy: 0.8053   Val Accuracy: 0.6681   Test Accuracy: 0.7641\n",
            "Epoch: 32   Loss: 22.0189   Time: 0.2288   Train Accuracy: 0.8109   Val Accuracy: 0.6765   Test Accuracy: 0.7641\n",
            "Epoch: 33   Loss: 21.7629   Time: 0.3110   Train Accuracy: 0.8042   Val Accuracy: 0.6765   Test Accuracy: 0.7808\n",
            "Epoch: 34   Loss: 21.5886   Time: 0.2279   Train Accuracy: 0.8131   Val Accuracy: 0.6848   Test Accuracy: 0.7628\n",
            "Epoch: 35   Loss: 21.3030   Time: 0.2491   Train Accuracy: 0.8164   Val Accuracy: 0.6848   Test Accuracy: 0.7628\n",
            "Epoch: 36   Loss: 21.0694   Time: 0.3095   Train Accuracy: 0.8164   Val Accuracy: 0.6848   Test Accuracy: 0.7724\n",
            "Epoch: 37   Loss: 21.0126   Time: 0.2340   Train Accuracy: 0.8198   Val Accuracy: 0.6765   Test Accuracy: 0.7808\n",
            "Epoch: 38   Loss: 20.6749   Time: 0.2344   Train Accuracy: 0.8211   Val Accuracy: 0.6848   Test Accuracy: 0.7891\n",
            "Epoch: 39   Loss: 20.5781   Time: 0.3048   Train Accuracy: 0.8253   Val Accuracy: 0.6765   Test Accuracy: 0.7891\n",
            "Epoch: 40   Loss: 20.3877   Time: 0.2278   Train Accuracy: 0.8300   Val Accuracy: 0.6848   Test Accuracy: 0.7891\n",
            "Epoch: 41   Loss: 20.1662   Time: 0.2277   Train Accuracy: 0.8276   Val Accuracy: 0.6765   Test Accuracy: 0.7891\n",
            "Epoch: 42   Loss: 19.9022   Time: 0.3028   Train Accuracy: 0.8300   Val Accuracy: 0.6750   Test Accuracy: 0.7987\n",
            "Epoch: 43   Loss: 19.7575   Time: 0.2420   Train Accuracy: 0.8367   Val Accuracy: 0.6946   Test Accuracy: 0.7891\n",
            "Epoch: 44   Loss: 19.5298   Time: 0.2517   Train Accuracy: 0.8344   Val Accuracy: 0.6863   Test Accuracy: 0.7987\n",
            "Epoch: 45   Loss: 19.3702   Time: 0.3275   Train Accuracy: 0.8389   Val Accuracy: 0.7044   Test Accuracy: 0.7891\n",
            "Epoch: 46   Loss: 19.0824   Time: 0.2628   Train Accuracy: 0.8400   Val Accuracy: 0.6961   Test Accuracy: 0.7987\n",
            "Epoch: 47   Loss: 18.9171   Time: 0.2484   Train Accuracy: 0.8367   Val Accuracy: 0.6946   Test Accuracy: 0.7987\n",
            "Epoch: 48   Loss: 18.7768   Time: 0.3285   Train Accuracy: 0.8444   Val Accuracy: 0.6961   Test Accuracy: 0.7987\n",
            "Epoch: 49   Loss: 18.5742   Time: 0.2580   Train Accuracy: 0.8456   Val Accuracy: 0.6946   Test Accuracy: 0.7987\n",
            "Epoch: 50   Loss: 18.3061   Time: 0.2377   Train Accuracy: 0.8511   Val Accuracy: 0.6961   Test Accuracy: 0.7987\n",
            "Epoch: 51   Loss: 18.2066   Time: 0.2678   Train Accuracy: 0.8467   Val Accuracy: 0.6946   Test Accuracy: 0.7987\n",
            "Epoch: 52   Loss: 18.0368   Time: 0.2420   Train Accuracy: 0.8522   Val Accuracy: 0.6961   Test Accuracy: 0.7987\n",
            "Epoch: 53   Loss: 17.8491   Time: 0.2581   Train Accuracy: 0.8544   Val Accuracy: 0.6946   Test Accuracy: 0.8071\n",
            "Epoch: 54   Loss: 17.6165   Time: 0.2459   Train Accuracy: 0.8567   Val Accuracy: 0.6961   Test Accuracy: 0.7987\n",
            "Epoch: 55   Loss: 17.5519   Time: 0.2533   Train Accuracy: 0.8578   Val Accuracy: 0.6863   Test Accuracy: 0.7987\n",
            "Epoch: 56   Loss: 17.2425   Time: 0.2423   Train Accuracy: 0.8600   Val Accuracy: 0.6961   Test Accuracy: 0.8167\n",
            "Epoch: 57   Loss: 17.0984   Time: 0.2392   Train Accuracy: 0.8578   Val Accuracy: 0.6863   Test Accuracy: 0.8071\n",
            "Epoch: 58   Loss: 16.9259   Time: 0.2336   Train Accuracy: 0.8622   Val Accuracy: 0.7142   Test Accuracy: 0.8167\n",
            "Epoch: 59   Loss: 16.8250   Time: 0.2345   Train Accuracy: 0.8622   Val Accuracy: 0.7029   Test Accuracy: 0.8083\n",
            "Epoch: 60   Loss: 16.6422   Time: 0.2622   Train Accuracy: 0.8711   Val Accuracy: 0.6961   Test Accuracy: 0.8167\n",
            "Epoch: 61   Loss: 18.4893   Time: 0.2355   Train Accuracy: 0.8600   Val Accuracy: 0.6946   Test Accuracy: 0.7712\n",
            "Epoch: 62   Loss: 16.8577   Time: 0.2388   Train Accuracy: 0.8722   Val Accuracy: 0.7044   Test Accuracy: 0.8071\n",
            "Epoch: 63   Loss: 16.5927   Time: 0.2391   Train Accuracy: 0.8689   Val Accuracy: 0.6946   Test Accuracy: 0.7904\n",
            "Epoch: 64   Loss: 16.1249   Time: 0.2440   Train Accuracy: 0.8733   Val Accuracy: 0.6946   Test Accuracy: 0.7987\n",
            "Epoch: 65   Loss: 15.7712   Time: 0.2459   Train Accuracy: 0.8733   Val Accuracy: 0.7044   Test Accuracy: 0.7891\n",
            "Epoch: 66   Loss: 15.5030   Time: 0.2366   Train Accuracy: 0.8778   Val Accuracy: 0.6961   Test Accuracy: 0.7987\n",
            "Epoch: 67   Loss: 15.2976   Time: 0.3155   Train Accuracy: 0.8789   Val Accuracy: 0.7044   Test Accuracy: 0.7987\n",
            "Epoch: 68   Loss: 15.2005   Time: 0.2497   Train Accuracy: 0.8800   Val Accuracy: 0.6961   Test Accuracy: 0.7891\n",
            "Epoch: 69   Loss: 14.9955   Time: 0.2440   Train Accuracy: 0.8856   Val Accuracy: 0.7044   Test Accuracy: 0.7891\n",
            "Epoch: 70   Loss: 14.9294   Time: 0.3160   Train Accuracy: 0.8900   Val Accuracy: 0.6863   Test Accuracy: 0.7891\n",
            "Epoch: 71   Loss: 14.7842   Time: 0.2346   Train Accuracy: 0.8889   Val Accuracy: 0.7029   Test Accuracy: 0.7891\n",
            "Epoch: 72   Loss: 14.6274   Time: 0.2486   Train Accuracy: 0.8900   Val Accuracy: 0.6863   Test Accuracy: 0.7891\n",
            "Epoch: 73   Loss: 14.3844   Time: 0.3024   Train Accuracy: 0.8944   Val Accuracy: 0.6946   Test Accuracy: 0.8071\n",
            "Epoch: 74   Loss: 14.3351   Time: 0.2413   Train Accuracy: 0.8922   Val Accuracy: 0.7127   Test Accuracy: 0.7891\n",
            "Epoch: 75   Loss: 14.0515   Time: 0.2307   Train Accuracy: 0.8967   Val Accuracy: 0.6946   Test Accuracy: 0.8071\n",
            "Epoch: 76   Loss: 14.0082   Time: 0.3527   Train Accuracy: 0.8989   Val Accuracy: 0.7029   Test Accuracy: 0.7891\n",
            "Epoch: 77   Loss: 13.8218   Time: 0.2389   Train Accuracy: 0.8944   Val Accuracy: 0.7127   Test Accuracy: 0.8071\n",
            "Epoch: 78   Loss: 13.5436   Time: 0.2351   Train Accuracy: 0.8989   Val Accuracy: 0.7127   Test Accuracy: 0.8071\n",
            "Epoch: 79   Loss: 13.4726   Time: 0.3174   Train Accuracy: 0.9033   Val Accuracy: 0.7127   Test Accuracy: 0.7987\n",
            "Epoch: 80   Loss: 13.3758   Time: 0.2558   Train Accuracy: 0.9067   Val Accuracy: 0.7127   Test Accuracy: 0.7987\n",
            "Epoch: 81   Loss: 13.2518   Time: 0.2415   Train Accuracy: 0.9011   Val Accuracy: 0.7127   Test Accuracy: 0.8071\n",
            "Epoch: 82   Loss: 12.9613   Time: 0.3194   Train Accuracy: 0.9044   Val Accuracy: 0.7044   Test Accuracy: 0.8071\n",
            "Epoch: 83   Loss: 12.6819   Time: 0.2551   Train Accuracy: 0.9078   Val Accuracy: 0.7127   Test Accuracy: 0.7987\n",
            "Epoch: 84   Loss: 12.4472   Time: 0.2378   Train Accuracy: 0.9122   Val Accuracy: 0.7127   Test Accuracy: 0.7987\n",
            "Epoch: 85   Loss: 12.3773   Time: 0.3402   Train Accuracy: 0.9111   Val Accuracy: 0.7127   Test Accuracy: 0.7904\n",
            "Epoch: 86   Loss: 12.2543   Time: 0.2518   Train Accuracy: 0.9089   Val Accuracy: 0.7044   Test Accuracy: 0.7987\n",
            "Epoch: 87   Loss: 12.1866   Time: 0.2600   Train Accuracy: 0.9122   Val Accuracy: 0.7029   Test Accuracy: 0.7987\n",
            "Epoch: 88   Loss: 11.8169   Time: 0.3486   Train Accuracy: 0.9211   Val Accuracy: 0.7142   Test Accuracy: 0.8071\n",
            "Epoch: 89   Loss: 11.6878   Time: 0.2364   Train Accuracy: 0.9222   Val Accuracy: 0.7044   Test Accuracy: 0.8167\n",
            "Epoch: 90   Loss: 11.5583   Time: 0.2374   Train Accuracy: 0.9222   Val Accuracy: 0.6848   Test Accuracy: 0.7904\n",
            "Epoch: 91   Loss: 11.3030   Time: 0.2911   Train Accuracy: 0.9222   Val Accuracy: 0.7127   Test Accuracy: 0.8071\n",
            "Epoch: 92   Loss: 11.0526   Time: 0.2377   Train Accuracy: 0.9267   Val Accuracy: 0.6863   Test Accuracy: 0.7891\n",
            "Epoch: 93   Loss: 10.9943   Time: 0.2334   Train Accuracy: 0.9267   Val Accuracy: 0.7127   Test Accuracy: 0.8071\n",
            "Epoch: 94   Loss: 10.8720   Time: 0.2237   Train Accuracy: 0.9300   Val Accuracy: 0.7044   Test Accuracy: 0.8071\n",
            "Epoch: 95   Loss: 10.6204   Time: 0.2325   Train Accuracy: 0.9333   Val Accuracy: 0.6931   Test Accuracy: 0.7987\n",
            "Epoch: 96   Loss: 10.3379   Time: 0.2292   Train Accuracy: 0.9356   Val Accuracy: 0.7044   Test Accuracy: 0.8071\n",
            "Epoch: 97   Loss: 10.3114   Time: 0.2537   Train Accuracy: 0.9378   Val Accuracy: 0.7029   Test Accuracy: 0.8071\n",
            "Epoch: 98   Loss: 10.0878   Time: 0.2503   Train Accuracy: 0.9311   Val Accuracy: 0.6946   Test Accuracy: 0.7987\n",
            "Epoch: 99   Loss: 9.8266   Time: 0.2832   Train Accuracy: 0.9389   Val Accuracy: 0.6848   Test Accuracy: 0.7987\n",
            "Epoch: 100   Loss: 9.6137   Time: 0.2489   Train Accuracy: 0.9433   Val Accuracy: 0.6848   Test Accuracy: 0.7987\n",
            "Epoch: 101   Loss: 9.5007   Time: 0.2620   Train Accuracy: 0.9400   Val Accuracy: 0.6946   Test Accuracy: 0.7987\n",
            "Epoch: 102   Loss: 9.3426   Time: 0.2295   Train Accuracy: 0.9444   Val Accuracy: 0.6848   Test Accuracy: 0.8071\n",
            "Epoch: 103   Loss: 9.2767   Time: 0.2356   Train Accuracy: 0.9433   Val Accuracy: 0.6765   Test Accuracy: 0.7987\n",
            "Epoch: 104   Loss: 9.1267   Time: 0.2440   Train Accuracy: 0.9422   Val Accuracy: 0.6765   Test Accuracy: 0.7904\n",
            "Epoch: 105   Loss: 8.9779   Time: 0.2473   Train Accuracy: 0.9411   Val Accuracy: 0.6848   Test Accuracy: 0.7737\n",
            "Epoch: 106   Loss: 8.7006   Time: 0.2504   Train Accuracy: 0.9489   Val Accuracy: 0.6765   Test Accuracy: 0.7987\n",
            "Epoch: 107   Loss: 8.6223   Time: 0.3256   Train Accuracy: 0.9489   Val Accuracy: 0.6848   Test Accuracy: 0.7904\n",
            "Epoch: 108   Loss: 8.4834   Time: 0.2610   Train Accuracy: 0.9489   Val Accuracy: 0.6765   Test Accuracy: 0.7795\n",
            "Epoch: 109   Loss: 8.4910   Time: 0.2384   Train Accuracy: 0.9500   Val Accuracy: 0.6848   Test Accuracy: 0.7987\n",
            "Epoch: 110   Loss: 8.3491   Time: 0.3334   Train Accuracy: 0.9556   Val Accuracy: 0.6848   Test Accuracy: 0.7904\n",
            "Epoch: 111   Loss: 8.1317   Time: 0.2419   Train Accuracy: 0.9511   Val Accuracy: 0.6750   Test Accuracy: 0.7808\n",
            "Epoch: 112   Loss: 7.9362   Time: 0.2449   Train Accuracy: 0.9533   Val Accuracy: 0.6750   Test Accuracy: 0.7808\n",
            "Epoch: 113   Loss: 7.8315   Time: 0.3110   Train Accuracy: 0.9544   Val Accuracy: 0.6750   Test Accuracy: 0.7904\n",
            "Epoch: 114   Loss: 7.5976   Time: 0.2525   Train Accuracy: 0.9578   Val Accuracy: 0.6652   Test Accuracy: 0.7724\n",
            "Epoch: 115   Loss: 7.4483   Time: 0.2442   Train Accuracy: 0.9589   Val Accuracy: 0.6931   Test Accuracy: 0.7808\n",
            "Epoch: 116   Loss: 7.4007   Time: 0.3133   Train Accuracy: 0.9611   Val Accuracy: 0.6833   Test Accuracy: 0.7712\n",
            "Epoch: 117   Loss: 7.2946   Time: 0.2456   Train Accuracy: 0.9689   Val Accuracy: 0.6833   Test Accuracy: 0.7712\n",
            "Epoch: 118   Loss: 7.2591   Time: 0.2685   Train Accuracy: 0.9656   Val Accuracy: 0.6931   Test Accuracy: 0.7628\n",
            "Epoch: 119   Loss: 7.3154   Time: 0.3386   Train Accuracy: 0.9633   Val Accuracy: 0.6819   Test Accuracy: 0.7808\n",
            "Epoch: 120   Loss: 6.8666   Time: 0.3336   Train Accuracy: 0.9722   Val Accuracy: 0.6735   Test Accuracy: 0.7821\n",
            "Epoch: 121   Loss: 6.7495   Time: 0.2364   Train Accuracy: 0.9711   Val Accuracy: 0.6833   Test Accuracy: 0.7628\n",
            "Epoch: 122   Loss: 6.6365   Time: 0.3138   Train Accuracy: 0.9744   Val Accuracy: 0.6833   Test Accuracy: 0.7712\n",
            "Epoch: 123   Loss: 6.7910   Time: 0.2340   Train Accuracy: 0.9678   Val Accuracy: 0.6917   Test Accuracy: 0.7641\n",
            "Epoch: 124   Loss: 6.6044   Time: 0.2335   Train Accuracy: 0.9722   Val Accuracy: 0.6750   Test Accuracy: 0.7808\n",
            "Epoch: 125   Loss: 6.5031   Time: 0.3340   Train Accuracy: 0.9756   Val Accuracy: 0.6667   Test Accuracy: 0.7808\n",
            "Epoch: 126   Loss: 6.2175   Time: 0.2367   Train Accuracy: 0.9778   Val Accuracy: 0.6750   Test Accuracy: 0.7808\n",
            "Epoch: 127   Loss: 6.2043   Time: 0.2538   Train Accuracy: 0.9767   Val Accuracy: 0.6485   Test Accuracy: 0.7808\n",
            "Epoch: 128   Loss: 6.2049   Time: 0.3150   Train Accuracy: 0.9756   Val Accuracy: 0.6652   Test Accuracy: 0.7808\n",
            "Epoch: 129   Loss: 6.0209   Time: 0.2226   Train Accuracy: 0.9789   Val Accuracy: 0.6765   Test Accuracy: 0.7808\n",
            "Epoch: 130   Loss: 5.8892   Time: 0.2383   Train Accuracy: 0.9800   Val Accuracy: 0.6402   Test Accuracy: 0.7641\n",
            "Epoch: 131   Loss: 6.1407   Time: 0.3240   Train Accuracy: 0.9744   Val Accuracy: 0.6735   Test Accuracy: 0.7724\n",
            "Epoch: 132   Loss: 5.6249   Time: 0.2272   Train Accuracy: 0.9811   Val Accuracy: 0.6569   Test Accuracy: 0.7808\n",
            "Epoch: 133   Loss: 6.0525   Time: 0.2215   Train Accuracy: 0.9722   Val Accuracy: 0.6569   Test Accuracy: 0.7724\n",
            "Epoch: 134   Loss: 5.7509   Time: 0.3073   Train Accuracy: 0.9733   Val Accuracy: 0.6569   Test Accuracy: 0.7808\n",
            "Epoch: 135   Loss: 5.5162   Time: 0.2465   Train Accuracy: 0.9778   Val Accuracy: 0.6485   Test Accuracy: 0.7724\n",
            "Epoch: 136   Loss: 5.3247   Time: 0.2305   Train Accuracy: 0.9800   Val Accuracy: 0.6485   Test Accuracy: 0.7808\n",
            "Epoch: 137   Loss: 5.2280   Time: 0.2880   Train Accuracy: 0.9844   Val Accuracy: 0.6569   Test Accuracy: 0.7724\n",
            "Epoch: 138   Loss: 5.1807   Time: 0.2188   Train Accuracy: 0.9833   Val Accuracy: 0.6485   Test Accuracy: 0.7808\n",
            "Epoch: 139   Loss: 4.9619   Time: 0.3068   Train Accuracy: 0.9844   Val Accuracy: 0.6569   Test Accuracy: 0.7641\n",
            "Epoch: 140   Loss: 4.8270   Time: 0.2360   Train Accuracy: 0.9856   Val Accuracy: 0.6569   Test Accuracy: 0.7724\n",
            "Epoch: 141   Loss: 4.6782   Time: 0.2428   Train Accuracy: 0.9889   Val Accuracy: 0.6554   Test Accuracy: 0.7724\n",
            "Epoch: 142   Loss: 4.6197   Time: 0.2274   Train Accuracy: 0.9856   Val Accuracy: 0.6637   Test Accuracy: 0.7641\n",
            "Epoch: 143   Loss: 4.4532   Time: 0.2299   Train Accuracy: 0.9867   Val Accuracy: 0.6554   Test Accuracy: 0.7724\n",
            "Epoch: 144   Loss: 4.3796   Time: 0.2434   Train Accuracy: 0.9856   Val Accuracy: 0.6735   Test Accuracy: 0.7641\n",
            "Epoch: 145   Loss: 4.3620   Time: 0.2503   Train Accuracy: 0.9889   Val Accuracy: 0.6569   Test Accuracy: 0.7628\n",
            "Epoch: 146   Loss: 4.1764   Time: 0.2333   Train Accuracy: 0.9856   Val Accuracy: 0.6637   Test Accuracy: 0.7724\n",
            "Epoch: 147   Loss: 4.2139   Time: 0.2485   Train Accuracy: 0.9900   Val Accuracy: 0.6637   Test Accuracy: 0.7808\n",
            "Epoch: 148   Loss: 4.1235   Time: 0.2712   Train Accuracy: 0.9900   Val Accuracy: 0.6637   Test Accuracy: 0.7808\n",
            "Epoch: 149   Loss: 4.0333   Time: 0.2419   Train Accuracy: 0.9878   Val Accuracy: 0.6735   Test Accuracy: 0.7808\n",
            "Epoch: 150   Loss: 4.1491   Time: 0.2305   Train Accuracy: 0.9867   Val Accuracy: 0.6637   Test Accuracy: 0.7808\n",
            "Epoch: 151   Loss: 4.0174   Time: 0.2367   Train Accuracy: 0.9878   Val Accuracy: 0.6735   Test Accuracy: 0.7808\n",
            "Epoch: 152   Loss: 3.9586   Time: 0.2284   Train Accuracy: 0.9911   Val Accuracy: 0.7015   Test Accuracy: 0.7904\n",
            "Epoch: 153   Loss: 4.3042   Time: 0.2223   Train Accuracy: 0.9833   Val Accuracy: 0.6721   Test Accuracy: 0.7712\n",
            "Epoch: 154   Loss: 3.8664   Time: 0.2338   Train Accuracy: 0.9878   Val Accuracy: 0.6917   Test Accuracy: 0.7821\n",
            "Epoch: 155   Loss: 4.0775   Time: 0.2326   Train Accuracy: 0.9800   Val Accuracy: 0.6917   Test Accuracy: 0.7724\n",
            "Epoch: 156   Loss: 4.2704   Time: 0.2778   Train Accuracy: 0.9822   Val Accuracy: 0.6833   Test Accuracy: 0.7821\n",
            "Epoch: 157   Loss: 4.0339   Time: 0.2347   Train Accuracy: 0.9811   Val Accuracy: 0.7015   Test Accuracy: 0.7987\n",
            "Epoch: 158   Loss: 4.3450   Time: 0.2921   Train Accuracy: 0.9833   Val Accuracy: 0.7015   Test Accuracy: 0.7821\n",
            "Epoch: 159   Loss: 4.0446   Time: 0.3334   Train Accuracy: 0.9822   Val Accuracy: 0.7015   Test Accuracy: 0.8000\n",
            "Epoch: 160   Loss: 4.2893   Time: 0.2326   Train Accuracy: 0.9822   Val Accuracy: 0.7015   Test Accuracy: 0.7904\n",
            "Epoch: 161   Loss: 4.3548   Time: 0.2380   Train Accuracy: 0.9756   Val Accuracy: 0.7015   Test Accuracy: 0.7904\n",
            "Epoch: 162   Loss: 4.4950   Time: 0.3119   Train Accuracy: 0.9822   Val Accuracy: 0.6931   Test Accuracy: 0.7808\n",
            "Epoch: 163   Loss: 4.9025   Time: 0.2369   Train Accuracy: 0.9722   Val Accuracy: 0.6833   Test Accuracy: 0.7987\n",
            "Epoch: 164   Loss: 5.4785   Time: 0.2500   Train Accuracy: 0.9667   Val Accuracy: 0.6833   Test Accuracy: 0.7724\n",
            "Epoch: 165   Loss: 8.6046   Time: 0.2913   Train Accuracy: 0.9576   Val Accuracy: 0.7015   Test Accuracy: 0.7904\n",
            "Epoch: 166   Loss: 6.6748   Time: 0.2465   Train Accuracy: 0.9633   Val Accuracy: 0.6917   Test Accuracy: 0.7712\n",
            "Epoch: 167   Loss: 5.8636   Time: 0.2399   Train Accuracy: 0.9622   Val Accuracy: 0.6917   Test Accuracy: 0.7904\n",
            "Epoch: 168   Loss: 6.6436   Time: 0.3047   Train Accuracy: 0.9622   Val Accuracy: 0.6721   Test Accuracy: 0.7712\n",
            "Epoch: 169   Loss: 8.1768   Time: 0.2372   Train Accuracy: 0.9653   Val Accuracy: 0.6833   Test Accuracy: 0.7641\n",
            "Epoch: 170   Loss: 17.6712   Time: 0.2575   Train Accuracy: 0.9089   Val Accuracy: 0.6819   Test Accuracy: 0.7737\n",
            "Epoch: 171   Loss: 13.9942   Time: 0.3129   Train Accuracy: 0.9222   Val Accuracy: 0.7113   Test Accuracy: 0.7891\n",
            "Epoch: 172   Loss: 11.1185   Time: 0.2372   Train Accuracy: 0.9267   Val Accuracy: 0.7113   Test Accuracy: 0.7808\n",
            "Epoch: 173   Loss: 9.5986   Time: 0.2276   Train Accuracy: 0.9300   Val Accuracy: 0.6721   Test Accuracy: 0.7558\n",
            "Epoch: 174   Loss: 10.8860   Time: 0.3075   Train Accuracy: 0.9267   Val Accuracy: 0.6569   Test Accuracy: 0.7641\n",
            "Epoch: 175   Loss: 10.8411   Time: 0.2302   Train Accuracy: 0.9278   Val Accuracy: 0.6569   Test Accuracy: 0.6686\n",
            "Epoch: 176   Loss: 11.1790   Time: 0.2298   Train Accuracy: 0.9322   Val Accuracy: 0.6206   Test Accuracy: 0.6423\n",
            "Epoch: 177   Loss: 11.0989   Time: 0.2469   Train Accuracy: 0.9211   Val Accuracy: 0.6250   Test Accuracy: 0.6064\n",
            "Epoch: 178   Loss: 12.7352   Time: 0.2438   Train Accuracy: 0.9178   Val Accuracy: 0.6554   Test Accuracy: 0.7378\n",
            "Epoch: 179   Loss: 9.4040   Time: 0.2364   Train Accuracy: 0.9389   Val Accuracy: 0.6819   Test Accuracy: 0.7878\n",
            "Epoch: 180   Loss: 6.6306   Time: 0.2324   Train Accuracy: 0.9622   Val Accuracy: 0.7015   Test Accuracy: 0.7628\n",
            "Epoch: 181   Loss: 6.9871   Time: 0.2370   Train Accuracy: 0.9564   Val Accuracy: 0.6735   Test Accuracy: 0.7724\n",
            "Epoch: 182   Loss: 9.2930   Time: 0.2344   Train Accuracy: 0.9478   Val Accuracy: 0.6569   Test Accuracy: 0.7295\n",
            "Epoch: 183   Loss: 5.5914   Time: 0.2272   Train Accuracy: 0.9611   Val Accuracy: 0.6373   Test Accuracy: 0.7212\n",
            "Epoch: 184   Loss: 5.5191   Time: 0.2313   Train Accuracy: 0.9733   Val Accuracy: 0.6721   Test Accuracy: 0.7615\n",
            "Epoch: 185   Loss: 4.1873   Time: 0.2525   Train Accuracy: 0.9844   Val Accuracy: 0.7015   Test Accuracy: 0.7808\n",
            "Epoch: 186   Loss: 3.9718   Time: 0.2227   Train Accuracy: 0.9844   Val Accuracy: 0.6735   Test Accuracy: 0.7724\n",
            "Epoch: 187   Loss: 3.6103   Time: 0.3395   Train Accuracy: 0.9844   Val Accuracy: 0.6652   Test Accuracy: 0.7724\n",
            "Epoch: 188   Loss: 3.1001   Time: 0.2276   Train Accuracy: 0.9956   Val Accuracy: 0.6721   Test Accuracy: 0.8071\n",
            "Epoch: 189   Loss: 2.8072   Time: 0.2550   Train Accuracy: 0.9933   Val Accuracy: 0.6721   Test Accuracy: 0.7891\n",
            "Epoch: 190   Loss: 2.8609   Time: 0.3010   Train Accuracy: 0.9944   Val Accuracy: 0.6819   Test Accuracy: 0.7641\n",
            "Epoch: 191   Loss: 2.7422   Time: 0.2274   Train Accuracy: 0.9944   Val Accuracy: 0.6721   Test Accuracy: 0.7821\n",
            "Epoch: 192   Loss: 2.4771   Time: 0.2344   Train Accuracy: 0.9944   Val Accuracy: 0.6721   Test Accuracy: 0.7808\n",
            "Epoch: 193   Loss: 2.4722   Time: 0.3328   Train Accuracy: 0.9967   Val Accuracy: 0.6819   Test Accuracy: 0.7724\n",
            "Epoch: 194   Loss: 2.4238   Time: 0.2338   Train Accuracy: 0.9967   Val Accuracy: 0.6819   Test Accuracy: 0.7904\n",
            "Epoch: 195   Loss: 2.3596   Time: 0.2282   Train Accuracy: 0.9967   Val Accuracy: 0.6819   Test Accuracy: 0.7808\n",
            "Epoch: 196   Loss: 2.3215   Time: 0.3090   Train Accuracy: 0.9967   Val Accuracy: 0.6721   Test Accuracy: 0.7904\n",
            "Epoch: 197   Loss: 2.3033   Time: 0.2382   Train Accuracy: 0.9967   Val Accuracy: 0.6721   Test Accuracy: 0.7904\n",
            "Epoch: 198   Loss: 2.2474   Time: 0.2547   Train Accuracy: 0.9967   Val Accuracy: 0.6721   Test Accuracy: 0.7808\n",
            "Epoch: 199   Loss: 2.2406   Time: 0.3282   Train Accuracy: 0.9967   Val Accuracy: 0.6721   Test Accuracy: 0.7808\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    sum_acc = 0\n",
        "    sum_loss = 0\n",
        "    for it, graph_list in enumerate(list_batch(train_dataset, batch_size)):\n",
        "        start = time.time()\n",
        "        gs, batch_idx = batch(graph_list)\n",
        "        num_graphs = len(graph_list)\n",
        "        labels = gs.globals - 1  # We subtract 1 to have labels in the range 0, 1\n",
        "        if epoch == 0 and it == 0:\n",
        "            # Initialize the model by performing a forward run with the first batch\n",
        "            network = hk.without_apply_rng(hk.transform(forward))\n",
        "            params_n = network.init(jax.random.PRNGKey(42), gs, batch_idx, create_new_batch, num_graphs, True)\n",
        "            opt_state = opt_init(params_n)\n",
        "        else:\n",
        "            out = network.apply(params_n, gs, batch_idx, create_new_batch, num_graphs, True)\n",
        "            acc = accuracy(out, labels).item()\n",
        "            params_n, opt_state, loss = update(params_n, opt_state, gs, batch_idx, create_new_batch, num_graphs, labels)\n",
        "            sum_acc += acc\n",
        "            sum_loss += loss.item()\n",
        "        end = time.time()\n",
        "\n",
        "    val_acc = eval(params_n, val_dataset, create_new_batch, batch_size)\n",
        "    test_acc = eval(params_n, test_dataset, create_new_batch, batch_size)\n",
        "\n",
        "    print(f\"Epoch: {epoch}   Loss: {sum_loss / (it + 1):.4f}   \"\n",
        "          f\"Time: {end - start:.4f}   \"\n",
        "          f\"Train Accuracy: {sum_acc / (it + 1):.4f}   \"\n",
        "          f\"Val Accuracy: {val_acc:.4f}   \"\n",
        "          f\"Test Accuracy: {test_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}